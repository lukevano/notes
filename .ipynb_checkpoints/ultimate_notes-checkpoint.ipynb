{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff658e19",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986a94c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_validate, cross_val_score\n",
    "# from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, RobustScaler, FunctionTransformer\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "# from sklearn.impute import SimpleImputer, KNNImputer\n",
    "# from sklearn.dummy import DummyRegressor\n",
    "# from sklearn.linear_model import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad43cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Loading data into Google Colab can be done by:\n",
    "# 1. Load from GitHub\n",
    "# Find in GitHub and View Raw\n",
    "url = 'copied_raw_GH_link'\n",
    "df1 = pd.read_csv(url)\n",
    "\n",
    "# 2. Loading directly into Colab from pwd\n",
    "!wget https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip # get data from server\n",
    "!unzip flowers-dataset.zip\n",
    "\n",
    "# 3. Moutning data in Google Drive to Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "# Then loading data from drive or folder\n",
    "def load_flowers_data(loading_method):\n",
    "    if loading_method == 'colab':\n",
    "        data_path = '/content/drive/My Drive/Deep_learning_data/flowers'\n",
    "    elif loading_method == 'direct':\n",
    "        data_path = 'flowers/'\n",
    "    classes = {'daisy':0, 'dandelion':1, 'rose':2}\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for (cl, i) in classes.items():\n",
    "        images_path = [elt for elt in os.listdir(os.path.join(data_path, cl)) if elt.find('.jpg')>0]\n",
    "        for img in tqdm(images_path[:300]):\n",
    "            path = os.path.join(data_path, cl, img)\n",
    "            if os.path.exists(path):\n",
    "                image = Image.open(path)\n",
    "                image = image.resize((256, 256))\n",
    "                imgs.append(np.array(image))\n",
    "                labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba885b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Getting jpgs into python\n",
    "from PIL import Image\n",
    "import requests\n",
    "shiba_url = \"https://c4.wallpaperflare.com/wallpaper/264/586/321/dog-shiba-inu-animals-wallpaper-preview.jpg\"\n",
    "shiba_image = Image.open(requests.get(shiba_url, stream=True).raw)\n",
    "shiba_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9b978",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ba5d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/ML_Houses_dataset.csv\")\n",
    "sample = orders.sample(1000, replace = True, random_state = 42)\n",
    "data[['BedroomAbvGr', 'KitchenAbvGr']].value_counts() # this will show how many houses have x bedrooms with y kitchens\n",
    "\n",
    "# Selecting columns with object and excluding these columns and seeing .nunique\n",
    "data.select_dtypes(include='object').nunique()\n",
    "data.select_dtypes(exclude='object').nunique()\n",
    "\n",
    "# Encoding cyclical continuous features\n",
    "seconds_in_day = 24*60*60\n",
    "df['sin_time'] = np.sin(2*np.pi*df.seconds/seconds_in_day)\n",
    "df['cos_time'] = np.cos(2*np.pi*df.seconds/seconds_in_day)\n",
    "df.drop('seconds', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6205a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc818757",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252678b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "plt.title(\"title\")\n",
    "plt.xlabel(\"label\")\n",
    "plt.ylabel(\"label\")\n",
    "plt.xticks([n1, n2, n3...], ['label_n1', label_n2, 'label_n3'])\n",
    "plt.yticks([n1, n2, n3...], ['label_n1', label_n2, 'label_n3'])\n",
    "plt.ylim(lower, higher)\n",
    "plt.xlim()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linewidth=0.5)\n",
    "plt.style.use('seaborn')\n",
    "with plt.style.context('seaborn'):\n",
    "    #graph_code_here\n",
    "    plt.show()\n",
    "ax = plt.gca()\n",
    "ax = pd.plot()\n",
    "ax.spines['right'].set_color(None)\n",
    "ax.spines['bottom'].set_position(('axes', 0.5))\n",
    "ax.spines['bottom'].set_position(('data', 750))\n",
    "\n",
    "# 3 methods of creating subplots:\n",
    "# 1. State-based interface- calling a plot then working on this\n",
    "plt.subplot(2,2,1)\n",
    "# 2. Object-orientated interface- calling functions on an instance of the graph\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.plot(years_x, coal_y, label=\"coal\")\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.plot(years_x, total_y, c='black')\n",
    "ax2.set_title('all energies')\n",
    "fig.suptitle('US electricity CO2 emissions')\n",
    "plt.show()\n",
    "# 3. Destructuring assignment\n",
    "# Destructuring initialization\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,3)) # axs is a (1,2) nd-array\n",
    "# First subplot\n",
    "axs[0].plot(years_x, coal_y, label=\"coal\")\n",
    "axs[0].plot(years_x, gas_y, label = \"gas\")\n",
    "axs[0].set_title('coal vs. gas')\n",
    "axs[0].legend()\n",
    "# Second subplot\n",
    "axs[1].plot(years_x, total_y, c='black')\n",
    "axs[1].set_title('all energies')\n",
    "# Global figure methods\n",
    "plt.suptitle('US electricity CO2 emissions')\n",
    "plt.show()\n",
    "\n",
    "# Creating subplots of different sizes\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Plotting from a df\n",
    "df1 = pd.DataFrame({ 'coal': coal_y, 'gas': gas_y }, index=years_x)\n",
    "df2 = pd.DataFrame({ 'total': total_y }, index=years_x)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "df1.plot(ax=ax1)\n",
    "df2.plot(ax=ax2)\n",
    "\n",
    "# How to plot the linear regression from a model on top of the data\n",
    "plt.scatter(X, y)\n",
    "# Get the axes so you know xlim\n",
    "axes = plt.gca()\n",
    "# Generate the x and y values then plot\n",
    "x_vals = np.array(axes.get_xlim())\n",
    "y_vals = model.intercept_ + model.coef_ * x_vals\n",
    "plt.plot(x_vals, y_vals, '--')\n",
    "\n",
    "# Different kinds of graph\n",
    "plt.scatter(data['views'], data['likes'], edgecolor='#333333', alpha=0.75);\n",
    "plt.bar(years_x, total_y);\n",
    "df.plot(kind='bar');\n",
    "plt.hist(x, bins=100);\n",
    "sns.histplot(tips_df['total_bill'], kde=True);\n",
    "sns.countplot(x=\"time\", hue=\"smoker\", data=tips_df, palette=\"name_here\");\n",
    "sns.catplot(x='day', y='total_bill', data=tips_df);\n",
    "sns.catplot(x='day', y='total_bill', data=tips_df, kind=\"box\");\n",
    "sns.scatterplot(x='petal length (cm)', y='petal width (cm)', data=iris_df, hue='species');\n",
    "sns.regplot(x='total_bill', y='tip', data=tips_df);\n",
    "# For FacetGrid Create a grid then plot:\n",
    "g = sns.FacetGrid(tips_df, col=\"time\", row=\"smoker\", hue=\"smoker\") \n",
    "g.map(sns.histplot, \"total_bill\");\n",
    "# Can also plot other graph types for FacetGrid\n",
    "g.map(sns.regplot, 'total_bill', 'tip')\n",
    "\n",
    "sns.pairplot(iris_df, hue=\"species\", corner = True);\n",
    "sns.heatmap(orders.corr(), cmap='coolwarm');\n",
    "sns.boxenplot(x='day',y='tip', data=df);\n",
    "sns.jointplot(data=df);\n",
    "# To plot two graphs with different y-scale on the same axes\n",
    "matplotlib.axes.Axes.twinx\n",
    "\n",
    "# RNN- You have n sequences with x obs each and the 2nd column is the value you want\n",
    "plt.title(\"# of persons under responsibility\")\n",
    "sns.histplot(X[:, :, 1].flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c0c67",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Maths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794a9af4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7fae47152bd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Inverse the matrix of A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Work out dot product of A_inv and B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Solve a linear matrix equation, or system of linear scalar equations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Work out determinant of an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.linalg.inv(A)  # Inverse the matrix of A\n",
    "np.dot(A_inv, B)  # Work out dot product of A_inv and B\n",
    "np.linalg.solve(A, B)  # Solve a linear matrix equation, or system of linear scalar equations\n",
    "np.linalg.det(a)  # Work out determinant of an array\n",
    "A.T  # This will transpose the matrix A\n",
    "np.hstack((a,b))  # Horizontal stack np columns\n",
    "np.vstack((X, X5))  # Vertical stack\n",
    "np.append(Y, Y_add, axis=0)\n",
    "np.subtract(predicted_price, prices)\n",
    "np.sqrt(x)\n",
    "np.linspace(0.2,0.3, num= 100)\n",
    "np.allclose(I, I4)  # is I the same as I4?\n",
    "MSEs.index(0.1)  # Show you the index of the value 0.1\n",
    "\n",
    "import scipy as sp\n",
    "from scipy.optimize import minimize  # to find local minima of a function\n",
    "minimize(function_name, x0, constraints=cons, bounds=bound) # x0= where you start in the function\n",
    "# scipy.optimize.shgo or scipy.optimize.dual_annealing iterative methods for global minima\n",
    "\n",
    "# fitting a line to quadratic curve\n",
    "from scipy.optimize import curve_fit\n",
    "def f(x, a, b, c):\n",
    "    return (a*x**2 + b*x + c)  # for quadratic curve\n",
    "coefs = optimize.curve_fit(f, x, y)  # fit f (curve function) to x and y values\n",
    "estimator_best = f(x, coefs[0], coefs[1], coefs[2])\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x, estimator_best, c= 'r')\n",
    "\n",
    "# interpolating values\n",
    "from scipy import interpolate\n",
    "f_interpolated = interpolate.interp1d(x,y, kind='linear')\n",
    "x_new = np.linspace(0, 10, 100)\n",
    "plt.scatter(x_new, f_interpolated(x_new))\n",
    "\n",
    "# Bernoulli process\n",
    "np.random.binomial(n=1, p=0.3, size=10)\n",
    "sns.distplot(np.random.binomial(n=1, p=0.3, size=10000), kde=False)\n",
    "\n",
    "pmf = stats.binom.pmf(x, n, p)\n",
    "plt.vlines(x, 0, pmf, linewidth=4)\n",
    "\n",
    "from scipy.stats import norm\n",
    "x = np.linspace(-10, 10, 100)\n",
    "#x = np.linspace(mu - 5*sigma, mu + 5*sigma, 100) # x points between 10 sigma\n",
    "plt.plot(x, stats.norm.pdf(x, mu, sigma))  # plot pdf for normally dist data\n",
    "plt.plot(x, stats.norm.cdf(x, mu, sigma))  # plot cdf for normally dist data\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "np.mean(means)\n",
    "np.std(means)\n",
    "skew(means)\n",
    "kurtosis(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7454e1ba",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Decision Science Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e70c7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Loading and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9ad33",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class Olist:\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        This function returns a Python dict.\n",
    "        Its keys should be 'sellers', 'orders', 'order_items' etc...\n",
    "        Its values should be pandas.DataFrame loaded from csv files\n",
    "        \"\"\"\n",
    "        # Hints: Build csv_path as \"absolute path\" in order to call this method from anywhere.\n",
    "        # Do not hardcode your path as it only works on your machine ('Users/username/code...')\n",
    "        # Use __file__ as absolute path anchor independant of your computer\n",
    "        # Make extensive use of `import ipdb; ipdb.set_trace()` to investigate what `__file__` variable is really\n",
    "        # Use os.path library to construct path independent of Unix vs. Windows specificities\n",
    "        \n",
    "        csv_path = '/Users/DrV/code/lukevano/data-challenges/04-Decision-Science/data/csv/'\n",
    "        file_names = [ i for i in os.listdir(csv_path) if '.csv' in i ]\n",
    "        key_names = [ i.replace('.csv', '').replace('_dataset', '').replace('olist_', '') for i in file_names]\n",
    "        file_path = [ os.path.join(csv_path, i) for i in file_names ]\n",
    "        data = {key: pd.read_csv(value) for key, value in zip(key_names, file_path)}\n",
    "        return data\n",
    "\n",
    "    def get_matching_table(self):\n",
    "        \"\"\"\n",
    "        01-01 > This function returns a matching table between\n",
    "        columns [ \"order_id\", \"review_id\", \"customer_id\", \"product_id\", \"seller_id\"]\n",
    "        \"\"\"\n",
    "        \n",
    "        data = self.get_data()\n",
    "        \n",
    "        merge_table = data['orders'].merge(data['order_items'], on= 'order_id', how= 'outer')\n",
    "        merge_table = merge_table.merge(data['order_reviews'], on= 'order_id', how= 'outer')\n",
    "        merge_table = merge_table.merge(data['customers'], on= 'customer_id', how= 'outer')\n",
    "        merge_table = merge_table.merge(data['sellers'], on= 'seller_id', how= 'outer')\n",
    "        merge_table = merge_table.merge(data['products'], on= 'product_id', how= 'outer')\n",
    "        matching_table = merge_table[['order_id', 'review_id', 'customer_id', 'product_id', 'seller_id']]\n",
    "        \n",
    "        return matching_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5964b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048dd678",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from olist.utils import haversine_distance\n",
    "from olist.data import Olist\n",
    "\n",
    "\n",
    "class Order:\n",
    "    '''\n",
    "    DataFrames containing all orders delivered as index,\n",
    "    and various properties of these orders as columns\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = Olist().get_data()\n",
    "        # The constructor of class Order assigns an attribute \".data\" to all new instances of Order\n",
    "        # i.e Order().data is defined\n",
    "\n",
    "    def get_wait_time(self, is_delivered=True):\n",
    "        \"\"\"\n",
    "        02-01 > Returns a DataFrame with:\n",
    "        [order_id, wait_time, expected_wait_time, delay_vs_expected, order_status]\n",
    "        filtering out non-delivered orders unless specified\n",
    "        \"\"\"\n",
    "        # Hint: Within this instance method, you have access to the instance of the class Order in the variable self\n",
    "        # make sure we don't create a \"view\" but a copy\n",
    "        orders = self.data['orders'].copy()\n",
    "\n",
    "        # filter delivered orders\n",
    "        if is_delivered:\n",
    "            orders = orders.query(\"order_status=='delivered'\").copy()\n",
    "\n",
    "        # handle datetime\n",
    "        orders.loc[:, 'order_delivered_customer_date'] = \\\n",
    "            pd.to_datetime(orders['order_delivered_customer_date'])\n",
    "        orders.loc[:, 'order_estimated_delivery_date'] = \\\n",
    "            pd.to_datetime(orders['order_estimated_delivery_date'])\n",
    "        orders.loc[:, 'order_purchase_timestamp'] = \\\n",
    "            pd.to_datetime(orders['order_purchase_timestamp'])\n",
    "\n",
    "        # compute delay vs expected\n",
    "        orders.loc[:, 'delay_vs_expected'] = \\\n",
    "            (orders['order_estimated_delivery_date'] -\n",
    "             orders['order_delivered_customer_date']) / np.timedelta64(24, 'h')\n",
    "\n",
    "        def handle_delay(x):\n",
    "            # We only want to keep delay where wait_time is longer than expected (not the other way around)\n",
    "            # This is what drives customer dissatisfaction!\n",
    "            if x < 0:\n",
    "                return abs(x)\n",
    "            return 0\n",
    "\n",
    "        orders.loc[:, 'delay_vs_expected'] = \\\n",
    "            orders['delay_vs_expected'].apply(handle_delay)\n",
    "\n",
    "        # compute wait time\n",
    "        orders.loc[:, 'wait_time'] = \\\n",
    "            (orders['order_delivered_customer_date'] -\n",
    "             orders['order_purchase_timestamp']) / np.timedelta64(24, 'h')\n",
    "\n",
    "        # compute expected wait time\n",
    "        orders.loc[:, 'expected_wait_time'] = \\\n",
    "            (orders['order_estimated_delivery_date'] -\n",
    "             orders['order_purchase_timestamp']) / np.timedelta64(24, 'h')\n",
    "\n",
    "        return orders[['order_id', 'wait_time', 'expected_wait_time',\n",
    "                       'delay_vs_expected', 'order_status']]\n",
    "\n",
    "    def get_review_score(self):\n",
    "        \"\"\"\n",
    "        02-01 > Returns a DataFrame with:\n",
    "        order_id, dim_is_five_star, dim_is_one_star, review_score\n",
    "        \"\"\"\n",
    "        # import data\n",
    "        reviews = self.data['order_reviews']\n",
    "\n",
    "        def dim_five_star(d):\n",
    "            if d == 5:\n",
    "                return 1\n",
    "            return 0\n",
    "\n",
    "        def dim_one_star(d):\n",
    "            if d == 1:\n",
    "                return 1\n",
    "            return 0\n",
    "\n",
    "        reviews.loc[:, 'dim_is_five_star'] =\\\n",
    "            reviews['review_score'].apply(dim_five_star)\n",
    "\n",
    "        reviews.loc[:, 'dim_is_one_star'] =\\\n",
    "            reviews['review_score'].apply(dim_one_star)\n",
    "\n",
    "        return reviews[['order_id', 'dim_is_five_star',\n",
    "                        'dim_is_one_star', 'review_score']]\n",
    "\n",
    "    def get_number_products(self):\n",
    "        \"\"\"\n",
    "        02-01 > Returns a DataFrame with:\n",
    "        order_id, number_of_products\n",
    "        \"\"\"\n",
    "\n",
    "        data = self.data\n",
    "        products = \\\n",
    "            data['order_items']\\\n",
    "            .groupby('order_id',\n",
    "                     as_index=False).agg({'order_item_id': 'count'})\n",
    "        products.columns = ['order_id', 'number_of_products']\n",
    "        return products\n",
    "\n",
    "    def get_number_sellers(self):\n",
    "        \"\"\"\n",
    "        02-01 > Returns a DataFrame with:\n",
    "        order_id, number_of_sellers\n",
    "        \"\"\"\n",
    "\n",
    "        data = self.data\n",
    "        sellers = \\\n",
    "            data['order_items']\\\n",
    "            .groupby('order_id')['seller_id'].nunique().reset_index()\n",
    "        sellers.columns = ['order_id', 'number_of_sellers']\n",
    "\n",
    "        return sellers\n",
    "\n",
    "    def get_price_and_freight(self):\n",
    "        \"\"\"\n",
    "        02-01 > Returns a DataFrame with:\n",
    "        order_id, price, freight_value\n",
    "        \"\"\"\n",
    "\n",
    "        data = self.data\n",
    "        price_freight = \\\n",
    "            data['order_items']\\\n",
    "            .groupby('order_id',\n",
    "                     as_index=False).agg({'price': 'sum',\n",
    "                                          'freight_value': 'sum'})\n",
    "\n",
    "        return price_freight\n",
    "\n",
    "    # Optional\n",
    "    def get_distance_seller_customer(self):\n",
    "        \"\"\"\n",
    "        02-01 > Returns a DataFrame with order_id\n",
    "        and distance between seller and customer\n",
    "        \"\"\"\n",
    "\n",
    "        # import data\n",
    "\n",
    "        data = self.data\n",
    "        matching_table = Olist().get_matching_table()\n",
    "\n",
    "        # Since one zipcode can map to multiple (lat, lng), take first one\n",
    "        geo = data['geolocation']\n",
    "        geo = geo.groupby('geolocation_zip_code_prefix',\n",
    "                          as_index=False).first()\n",
    "\n",
    "        # Select sellers and customers\n",
    "        sellers = data['sellers']\n",
    "        customers = data['customers']\n",
    "\n",
    "        # Merge geo_location for sellers\n",
    "        sellers_mask_columns = ['seller_id', 'seller_zip_code_prefix',\n",
    "                                'seller_city', 'seller_state',\n",
    "                                'geolocation_lat', 'geolocation_lng']\n",
    "\n",
    "        sellers_geo = sellers.merge(\n",
    "            geo,\n",
    "            how='left',\n",
    "            left_on='seller_zip_code_prefix',\n",
    "            right_on='geolocation_zip_code_prefix')[sellers_mask_columns]\n",
    "\n",
    "        # Merge geo_location for customers\n",
    "        customers_mask_columns = ['customer_id', 'customer_zip_code_prefix',\n",
    "                                  'customer_city', 'customer_state',\n",
    "                                  'geolocation_lat', 'geolocation_lng']\n",
    "\n",
    "        customers_geo = customers.merge(\n",
    "            geo,\n",
    "            how='left',\n",
    "            left_on='customer_zip_code_prefix',\n",
    "            right_on='geolocation_zip_code_prefix')[customers_mask_columns]\n",
    "\n",
    "        # Use the matching table and merge customers and sellers\n",
    "        matching_geo = matching_table.merge(sellers_geo,\n",
    "                                            on='seller_id')\\\n",
    "                                     .merge(customers_geo,\n",
    "                                            on='customer_id',\n",
    "                                            suffixes=('_seller',\n",
    "                                                      '_customer'))\n",
    "        # Remove na()\n",
    "        matching_geo = matching_geo.dropna()\n",
    "\n",
    "        matching_geo.loc[:, 'distance_seller_customer'] =\\\n",
    "            matching_geo.apply(\n",
    "                lambda row: haversine_distance(\n",
    "                    row['geolocation_lng_seller'],\n",
    "                    row['geolocation_lat_seller'],\n",
    "                    row['geolocation_lng_customer'],\n",
    "                    row['geolocation_lat_customer']), axis=1)\n",
    "        # Since an order can have multiple sellers,\n",
    "        # return the average of the distance per order\n",
    "        order_distance =\\\n",
    "            matching_geo.groupby(\n",
    "                'order_id',\n",
    "                as_index=False).agg({'distance_seller_customer': 'mean'})\n",
    "\n",
    "        return order_distance\n",
    "\n",
    "    def get_training_data(self, is_delivered=True,\n",
    "                          with_distance_seller_customer=False):\n",
    "        \"\"\"\n",
    "        02-01 > Returns a clean DataFrame (without NaN), with the following\n",
    "        columns: [order_id, wait_time, expected_wait_time, delay_vs_expected,\n",
    "        dim_is_five_star, dim_is_one_star, review_score, number_of_products,\n",
    "        number_of_sellers, price, freight_value, distance_customer_seller]\n",
    "        \"\"\"\n",
    "        # Hint: make sure to re-use your instance methods defined above\n",
    "        training_set =\\\n",
    "            self.get_wait_time(is_delivered)\\\n",
    "                .merge(\n",
    "                self.get_review_score(), on='order_id'\n",
    "               ).merge(\n",
    "                self.get_number_products(), on='order_id'\n",
    "               ).merge(\n",
    "                self.get_number_sellers(), on='order_id'\n",
    "               ).merge(\n",
    "                self.get_price_and_freight(), on='order_id'\n",
    "               )\n",
    "        # Skip heavy computation of distance_seller_customer unless specified\n",
    "        if with_distance_seller_customer:\n",
    "            training_set = training_set.merge(\n",
    "                self.get_distance_seller_customer(), on='order_id')\n",
    "\n",
    "        return training_set.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6224c317",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098b2c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "# Instantiate a normal curve from your data then give you pdf or cdf for an x value\n",
    "mu_estim = stats.norm(mean, sd)\n",
    "mu_estim.pdf(x)\n",
    "mu_estim.cdf(x)\n",
    "# Instantiate the normal curve and give you the pdf/cdf for this x value\n",
    "stats.norm.pdf(x, mu, sigma)\n",
    "stats.norm.cdf(x, mu, sigma)\n",
    "\n",
    "from scipy import stats\n",
    "stats.zscore()\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "model = smf.ols(formula = 'weight ~ horsepower', data = data).fit()\n",
    "model.params\n",
    "model.rsquared\n",
    "model.summary()\n",
    "model.predict(zscore_dict)\n",
    "\n",
    "# Plotting residuals to view distribution\n",
    "model = smf.ols(formula = 'weight ~ horsepower', data = data).fit()\n",
    "predicted_weights = model.predict(mpg['horsepower'])\n",
    "residuals = predicted_weights - mpg['weight']\n",
    "sns.histplot(residuals, kde=True, edgecolor='w')\n",
    "# Plotting to see if equal variance- if equal values will fit between 2 parallel lines\n",
    "sns.scatterplot(x=predicted_weights, y=residuals)\n",
    "plt.xlabel('Predicted weight')\n",
    "plt.ylabel('Residual weight')\n",
    "\n",
    "# Multiple features\n",
    "model2 = smf.ols(formula='weight ~ horsepower + cylinders', data=mpg).fit()\n",
    "# Removed the intercept with -1\n",
    "model3 = smf.ols(formula='weight ~ horsepower + cylinders -1', data=mpg).fit()\n",
    "\n",
    "# Logistic regression for one target\n",
    "model1 = smf.logit(formula='survived ~ 1', data=titanic).fit();\n",
    "# Logistic regression for one target and a catagorical feature and two continuous\n",
    "model2 = smf.logit(formula='survived ~ fare + C(sex) + age', data=titanic).fit();\n",
    "# One Vs One Logistic regression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "ovo_log_model = OneVsOneClassifier(LogisticRegression());\n",
    "# One Vs Rest Logistic regression\n",
    "ovr_log_model = LogisticRegression(multi_class='ovr');\n",
    "\n",
    "# Measuring VIF for feature at index 0\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "vif(car_df.values, 0)\n",
    "# Measuring VIF for all features and putting this in a df\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "df = pd.DataFrame()\n",
    "df[\"vif_index\"] = [vif(cars_df.values, i) for i in range(cars_df.shape[1])]\n",
    "df[\"features\"] = cars_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c40d9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d098fb6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59ff70",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1. Identify missing values and drop rows/columns or impute\n",
    "data.isnull.sum()\n",
    "df.replace('?', np.nan, inplace = True)\n",
    "df.dropna()  # drop rows with nan\n",
    "df.drop(columns = 'col_name', inplace = True)  # drop column\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"mean\") # median, most_frequent, constant\n",
    "imputer.fit(data[['Pesos']]) # fit the data into the transformer\n",
    "data['Pesos'] = imputer.transform(data[['Pesos']]) # transform the data using the model\n",
    "imputer.statistics_ # The mean is stored in the transformer's memory\n",
    "\n",
    "# 2. Scaling (using standardising or normalising) data to manage outliers\n",
    "data[['GrLivArea']].boxplot()  # Look for outlier and use step 1 on these if needed\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "scaler = StandardScaler()  # Best if normal distribution\n",
    "scaler = RobustScaler()  # Less sensitive to outliers\n",
    "normalise_scaler = MinMaxScaler()  # Most sensitive to outliers. Between 0 and 1\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "# Get names of numerical columns:\n",
    "num_features = list(X.select_dtypes(exclude='object').columns)\n",
    "\n",
    "# 3. Encoding non-numerical data\n",
    "# For each unique value in a column ohe will make a new column. Do not make too many!\n",
    "data.select_dtypes(include='object').nunique()\n",
    "data.select_dtypes(include='object').nunique().sum() # if 252 values need 2520 observations at least\n",
    "# For features that are non-binary\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False) # Instanciate encoder and need to make sparse false\n",
    "ohe.fit(data[['Alley']]) # Fit encoder\n",
    "alley_encoded = ohe.transform(data[['Alley']]) # Encode alley\n",
    "data[\"Grvl\"],data[\"NoAlley\"],data['Pave'] = alley_encoded.T '''Transpose encoded Alley back\n",
    "into dataframe, making a column for each category'''\n",
    "# If binary or target use label_encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df['gender'])\n",
    "df['gender_encoder'] = label_encoder.transform(df['gender'])\n",
    "\n",
    "# 4. Making categorical data ordinal\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# Define specific order for features\n",
    "# Note: if you change this order, it will change the output for .transform()\n",
    "feature_A_sorted_values = ['bad', 'average', 'good'] \n",
    "feature_B_sorted_values = ['dirty', 'clean', 'new']\n",
    "encoder = OrdinalEncoder(\n",
    "    categories=[\n",
    "        feature_A_sorted_values,\n",
    "        feature_B_sorted_values\n",
    "    ],\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "# Example data\n",
    "XX = [['good', 'dirty'], ['bad', 'new'], ['average', 'clean'],]\n",
    "encoder.fit(XX)\n",
    "encoder.transform([['bad', \"dirty\"], ['good', 'new'], ['average', 'oooops never seen this label before']])\n",
    "# Gives an array with 0, 1, 2, and -1 in\n",
    "\n",
    "# 5. Over/under sampling or synthesis of new instances from minority group\n",
    "\n",
    "# 6. Discretizing\n",
    "# Convert sale price to cheap or expensive\n",
    "data['SalePriceBinary'] = pd.cut(x = data['SalePrice'],bins=[data['SalePrice'].min()-1,\\\n",
    " data['SalePrice'].mean(), data['SalePrice'].max()+1], labels=['cheap', 'expensive'])\n",
    "\n",
    "# 7. Feature creation= make new features (e.g. joining or augmenting features [BMI])\n",
    "\n",
    "# 8. Feature selection\n",
    "# Remove features with high correlation- univariate analysis with corr df:\n",
    "corr = data.corr()\n",
    "corr_df = corr.unstack().reset_index() # Unstack correlation matrix \n",
    "corr_df.columns = ['feature_1','feature_2', 'correlation'] # rename columns\n",
    "corr_df.sort_values(by=\"correlation\",ascending=False, inplace=True) # sort by correlation\n",
    "corr_df = corr_df[corr_df['feature_1'] != corr_df['feature_2']] # Remove self correlation\n",
    "# Selection through multivariate analysis- showing the absolute value that feat contributes to r2 score\n",
    "from sklearn.inspection import permutation_importance\n",
    "log_model = LogisticRegression().fit(X, y) # Fit model\n",
    "permutation_score = permutation_importance(log_model, X, y, n_repeats=10) # Perform Permutation\n",
    "importance_df = pd.DataFrame(np.vstack((X.columns,\\\n",
    "        permutation_score.importances_mean)).T) # Unstack results\n",
    "importance_df.columns=['feature','score decrease']\n",
    "importance_df.sort_values(by=\"score decrease\", ascending = False) # Order by importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f6c9f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd6a1a1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-2-029afcdaefb9>, line 54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-029afcdaefb9>\"\u001b[0;36m, line \u001b[0;32m54\u001b[0m\n\u001b[0;31m    cv_results = cross_validate(model, X, y, cv=5, scoring=['max_error','r2',\\\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "baseline_model = DummyRegressor(strategy= 'quantile') # between 0 and 1\n",
    "# mean, median, quantile, or user specified constant can be used\n",
    "\n",
    "# Or just define X and y to whole dataset if not wanting to split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split the data into train and test (random)\n",
    "train_data, test_data = train_test_split(data_df, test_size=0.3)\n",
    "# split the data into train and test (same each time- only do this for sharing)\n",
    "train_data, test_data = train_test_split(data_df, test_size=0.3, random_state=42)\n",
    "# Ready Xs and ys\n",
    "X_train = train_data[['GrLivArea']]\n",
    "y_train = train_data['SalePrice']\n",
    "X_test = test_data[['GrLivArea']]\n",
    "y_test = test_data['SalePrice']\n",
    "# Or select the X and Y only\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "# Training, test, validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.20)\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(X_train, y_train, test_size=0.20)\n",
    "# Or use KFold get a list of indexes of the data (split into 3 groups)\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "for train_index, test_index in kf.split(X): # train will be 2/3, test 1/3\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y_cat[train_index], y_cat[test_index]\n",
    "# Instanciate the model\n",
    "model = LinearRegression()\n",
    "# Train the model on the Training data\n",
    "model.fit(X_train, y_train)\n",
    "# Evaluate the model's performance (R square) on the Testing data\n",
    "model.score(X_test,y_test)\n",
    "# View the model's slope (a)\n",
    "model.coef_ \n",
    "# View the model's intercept (b)\n",
    "model.intercept_\n",
    "#  Predict on new data\n",
    "model.predict([[150000, 3, 2, 8]])\n",
    "\n",
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X = data[['GrLivArea']]\n",
    "y = data['floots']\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "model.score(X, y)\n",
    "# Will give the most likely category (e.g. 1 or 0)\n",
    "model.predict([[1000]])\n",
    "# Predicts the proba of being in each possible category\n",
    "model.predict_proba([1000])\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "model = LinearRegression()\n",
    "X = data[['GrLivArea']]\n",
    "y = data['floots']\n",
    "# 5-Fold Cross validate model\n",
    "cv_results = cross_validate(model, X, y, cv=5)\n",
    "cv_results['test_score']\n",
    "# Mean of scores\n",
    "cv_results['test_score'].mean()\n",
    "# Get regression/classification metrics of the CV model (see documentation for metrics)\n",
    "cv_results = cross_validate(model, X, y, cv=5, scoring=['max_error','r2',\\ \n",
    "    'neg_mean_absolute_error', 'neg_mean_squared_error', 'f1', 'recall'])\n",
    "\n",
    "# Gives you the CV score only\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(LogisticRegression(), X, y, cv=10)\n",
    "scores.mean()\n",
    "# Gives you a target predictions based on X using the model created\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "predictions = cross_val_predict(model, X_test, cv= 10)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve\n",
    "# Need to have all at least one outcome for each target category (smaller sizes may not work)\n",
    "train_sizes = [25,50,75,100,250,500,750,1000,1150]\n",
    "# Get train scores (R2), train sizes, and validation scores using `learning_curve`\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator=LinearRegression(), X=X, y=y, train_sizes=train_sizes, cv=5)\n",
    "# Take the mean of cross-validated train scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "#plt.plot(train_sizes, train_scores_mean, label = 'Training score')\n",
    "#plt.plot(train_sizes, test_scores_mean, label = 'Test score')\n",
    "#plt.ylabel('r2 score', fontsize = 14)\n",
    "#plt.xlabel('Training set size', fontsize = 14)\n",
    "#plt.title('Learning curves', fontsize = 18, y = 1.03)\n",
    "#plt.legend()\n",
    "\n",
    "# Making confusion matrix\n",
    "results_df = pd.DataFrame({\"actual\": y, \"predicted\": y_pred})\n",
    "confusion_matrix = pd.crosstab(index= results_df['actual'],columns = results_df['predicted'])\n",
    "# Plotting confusion matrix- train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "model = LogisticRegression(max_iter= 1000).fit(X_train,y_train)\n",
    "plot_confusion_matrix(model, X_test, y_test)\n",
    "# Regression metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error\n",
    "import math\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "rsquared = r2_score(y, y_pred)\n",
    "max_error = max_error(y, y_pred)\n",
    "print('MSE =', round(mse, 2))\n",
    "print('RMSE =', round(rmse, 2))\n",
    "print('MAE =', round(mae, 2))\n",
    "print('R2 =', round(rsquared, 2))\n",
    "print('Max Error =', round(max_error, 2))\n",
    "# Classification metrics- pos_label= 1 at default so change if looking at 0\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Accuracy =', round(accuracy_score(y, y_pred, pos_label = 1), 2)) # Accuracy\n",
    "print('Precision =', round(precision_score(y, y_pred), 2)) # Precision\n",
    "print('Recall =', round(recall_score(y, y_pred), 2)) # Recall\n",
    "print('F1 score =', round(f1_score(y, y_pred), 2)) # F1 score\n",
    "# Automatically plot classification metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y, cv_pred))\n",
    "\n",
    "# Precision recall curve for recall 80% (identifying expensive policies)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "model = LogisticRegression()\n",
    "# Predict class probabilties\n",
    "data['prob_cheap'], data['prob_expensive'] =cross_val_predict(model, X,\\\n",
    "    data['price_range_encoded'], cv=5, method='predict_proba').T\n",
    "# Precision recall data\n",
    "precision, recall, threshold = precision_recall_curve(data['price_range_encoded'],\n",
    "                                                      data['proba_expensive']) \n",
    "print(f'precision- {precision[:5]}')\n",
    "print(f'recall- {recall[:5]}')\n",
    "print(f'threshold- {threshold[:5]}')\n",
    "# Plot tradeoff\n",
    "plt.plot(scores['recall'],scores['precision'])\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('recall')\n",
    "# Find threshold that guarantees 0.8 recall score\n",
    "scores[scores['recall'] >= 0.8].threshold.max()\n",
    "# \n",
    "model = LogisticRegression()\n",
    "model.fit(X, data['price_range_encoded'])\n",
    "def custom_predict(X, custom_threshold):\n",
    "    probs = model.predict_proba(X) # Get likelihood of each sample being classified as 0 or 1\n",
    "    expensive_probs = probs[:, 1] # Only keep expensive likelihoods (1) \n",
    "    return (expensive_probs > custom_threshold).astype(int) # Boolean outcome converted to 0 or 1\n",
    "updated_preds = custom_predict(X=X, custom_threshold=0.305539) # Update predictions \n",
    "print(recall_score(data['price_range_encoded'], updated_preds)) # Rerun recall\n",
    "print(precision_score(data['price_range_encoded'], updated_preds)) # Rerun precision\n",
    "print(f1_score(data['price_range_encoded'], updated_preds)) # Rerun f1\n",
    "\n",
    "# ROC AUC\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Extract associated metrics and thresholds then work out AUC\n",
    "fpr, tpr, thresholds = roc_curve(data['price_range_encoded'], data['proba_expensive'])\n",
    "auc_score = roc_auc_score(y, y_pred)\n",
    "\n",
    "# KNN- use MinMaxScaler()\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "model = KNeighborsRegressor(n_neighbors=2)\n",
    "\n",
    "# Solvers and loss functions\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
    "lin_reg_sgd = SGDRegressor(loss='squared_loss') # OLS loss function solved by SGD\n",
    "SGDRegressor(loss='huber') # Huber loss function, non-OLS linear regression\n",
    "SGDClassifier(loss='log') # eq. to Logit\n",
    "SGDClassifier(loss='hinge') # eq. to SVC\n",
    "\n",
    "# Using regulatization penalties to reduce model complexity (coef impact)\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "linreg = LinearRegression().fit(X, y)\n",
    "ridge = Ridge(alpha=0.2).fit(X, y)\n",
    "lasso = Lasso(alpha=0.2).fit(X, y)\n",
    "# Making a df comparing coef using regularization penalties\n",
    "coefs = pd.DataFrame({\n",
    "    \"coef_linreg\": pd.Series(linreg.coef_, index = X.columns),\n",
    "    \"coef_ridge\": pd.Series(ridge.coef_, index = X.columns),\n",
    "    \"coef_lasso\": pd.Series(lasso.coef_, index= X.columns)})\\\n",
    "coefs\\\n",
    "    .applymap(lambda x: int(x))\\\n",
    "    .style.applymap(lambda x: 'color: red' if x == 0 else 'color: black')\n",
    "\n",
    "# Using both Lasso (L1) and Ridge (L2)\n",
    "from sklearn.linear_model import ElasticNet\n",
    "model = ElasticNet(alpha=1, l1_ratio=0.2)  # Ratio between L1 and L2\n",
    "\n",
    "# Manually finding best hyperparam values- Long method\n",
    "alphas = [0.01, 0.1, 1] # L1 + L2 \n",
    "l1_ratios = [0.2, 0.5, 0.8] # L1 / L2 ratio\n",
    "# create all combinations [(0.01, 0.2), (0.01, 0.5), (...)]\n",
    "import itertools\n",
    "hyperparams = itertools.product(alphas, l1_ratios) # will create a list of all the possible combinations (len= 9)\n",
    "# Train and CV-score model for each combination\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selectionl import cross_val_score\n",
    "for hyperparam in hyperparams:\n",
    "    alpha = hyperparam[0]\n",
    "    l1_ratio = hyperparam[1]\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "    r2 = cross_val_score(model, X_train, y_train, cv=5).mean()\n",
    "    print(f\"alpha: {alpha}, l1_ratio: {l1_ratio},   r2: {r2}\")\n",
    "    \n",
    "# Finding best hyperparam values using grid search\n",
    "from sklearn.linear_model import ElasticNet # Linear regression with combined L1 and L2 priors as regularizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()  # Best if normal distribution\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "model = ElasticNet()  # Can use other models (like KNN etc)\n",
    "grid = {'alpha': [0.01, 0.1, 1], \n",
    "        'l1_ratio': [0.2, 0.5, 0.8]} # Spec the params to be changed in the model\n",
    "search = GridSearchCV(model, grid, \n",
    "                           scoring = 'r2',\n",
    "                           cv = 5,\n",
    "                           n_jobs=-1 # paralellize computation\n",
    "                          ) \n",
    "search.fit(X_train_scaled,y_train);\n",
    "# Finding best hyperparam values using grid/random search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import ElasticNet # Linear regression with combined L1 and L2 priors as regularizer\n",
    "from scipy import stats\n",
    "model = ElasticNet()  # Can use other models (like KNN etc)\n",
    "grid = {'l1_ratio': stats.uniform(0, 1), 'alpha': [0.001, 0.01, 0.1, 1]} \n",
    "''' Spec the param to be changed in the model. Pass distribution or long list.\n",
    "stats.uniform creates a pdf of uniform distribution between 0 and 1\n",
    "will randomly sample between 0 and 1 for the L1 ratio '''\n",
    "search = RandomizedSearchCV(model, grid, \n",
    "                            scoring='r2',\n",
    "                            n_iter=100,  # number of draws (must be shorter than list)\n",
    "                            cv=5, n_jobs=-1)\n",
    "search.fit(X_train_scaled,y_train);\n",
    "# Fit data to Grid/Random Search\n",
    "search.fit(X_train_scaled, y_train)\n",
    "search.best_estimator_\n",
    "# Best score\n",
    "search.best_score_\n",
    "# Best Params\n",
    "search.best_params_\n",
    "# Best estimator (can made predictions from this model)\n",
    "search.best_estimator_\n",
    "# All the results\n",
    "pd.DataFrame(search.cv_results_)\n",
    "\n",
    "# Choosing hyperparameter values to test\n",
    "from scipy import stats\n",
    "dist = stats.norm(10, 2) # if you have a best guess (say: 10)\n",
    "dist = stats.randint(1,100) # if you have no idea\n",
    "dist = stats.uniform(1, 100) # same output as above but will give floats\n",
    "dist = stats.loguniform(0.01, 1) # Coarse grain search- Will show you more results closer to 0\n",
    "r = dist.rvs(size=10000) # Random draws\n",
    "\n",
    "# SVM\n",
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "svc_10 = SVC(kernel='linear', C=10) # Linear kernel\n",
    "# equivalent but with SGD solver\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "svc_bis = SGDClassifier(loss='hinge', penalty='l2', alpha=1/10)\n",
    "# Polynomial kernel\n",
    "svm = SVC(kernel='poly', degree=2, C=100)\n",
    "# SVM Regression\n",
    "from sklearn.svm import SVR\n",
    "regressor = SVR(epsilon=0.1, C=1, kernel='linear')\n",
    "# Plot your instantiated classifier\n",
    "from utils.plots import plot_decision_regions\n",
    "plot_decision_regions(X, y, classifier=svm_10) # svm_10 is the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f32d7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa9d86",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a233d3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config; set_config(display='diagram') # Visualizing pipelines in HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "# Load dataset\n",
    "data = sns.load_dataset('titanic').sample(frac=1)\n",
    "print(data.shape)\n",
    "data.head()\n",
    "\n",
    "# Make target and feature df\n",
    "y = data.pop('survived')\n",
    "X = data\n",
    "\n",
    "# Do train/test split\n",
    "X_train, y_train, X_test, y_test = train_test_split(X,y,test_size=0.3)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Look at dtypes and values\n",
    "X_train.dtypes\n",
    "X_train.alone.unique()\n",
    "\n",
    "# Identify num and cat features\n",
    "num_features = ['age', 'sibsp', 'parch', 'fare', 'weight', 'height']\n",
    "cat_features = ['pclass', 'sex', 'embarked', 'class', 'deck', 'alone']\n",
    "\n",
    "# Check the distribution of num_features\n",
    "data[num_features].boxplot()\n",
    "\n",
    "# Build Pipeline Numerical\n",
    "num_pipeline = Pipeline([\n",
    "    ('n_imputer', SimpleImputer()),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "num_pipeline\n",
    "\n",
    "# Check num_pipeline has worked\n",
    "# num_pipeline.fit(X_train[num_features])\n",
    "# num_pipeline.fit_transform(X_train[num_features])\n",
    "\n",
    "# Build Pipeline Categorical\n",
    "cat_pipeline = Pipeline([\n",
    "    ('c_imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoding ', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "])\n",
    "cat_pipeline\n",
    "\n",
    "# Check cat_pipeline has worked\n",
    "# cat_pipeline.fit(X_train[cat_features])\n",
    "# cat_pipeline.fit_transform(X_train[cat_features])\n",
    "\n",
    "# Feature creation using FunctionTransformer (for stateless transformations only)\n",
    "# See advanced below for stateful transformations\n",
    "# Encapsulate a function into a transformer object to apply to columns\n",
    "bmi_constructor = FunctionTransformer(lambda X: \\\n",
    "            pd.DataFrame(X[\"weight\"] * X[\"height\"] ** 2))\n",
    "\n",
    "bmi_pipeline = Pipeline([\n",
    "    ('computing_bmi', bmi_constructor),\n",
    "    ('scaler_volume', StandardScaler())\n",
    "])\n",
    "bmi_pipeline\n",
    "\n",
    "# # Custom stateful transformer (advanced)\n",
    "# from sklearn.base import TransformerMixin, BaseEstimator\n",
    "# class CustomScaler(TransformerMixin, BaseEstimator):\n",
    "# # CustomScaler will center data around its mean and shrink it by a fixed factor\n",
    "# # TransformerMixin generates a fit_transform method from fit and transform\n",
    "# # BaseEstimator generates get_params and set_params methods\n",
    "#     def __init__(self, shrink_factor):\n",
    "#         self.shrink_factor = shrink_factor\n",
    "#     def fit(self, X, y=None):\n",
    "#         self.means = X.mean()\n",
    "#         return self\n",
    "#     def transform(self, X, y=None):\n",
    "#         X_transformed = (X - self.means) / self.shrink_factor\n",
    "#         # Return result as dataframe for integration into ColumnTransformer\n",
    "#         return pd.DataFrame(X_transformed)\n",
    "# # The CustomScaler can then be used like any other transformer\n",
    "# custom_scaler = CustomScaler(shrink_factor=3)\n",
    "# custom_scaler.fit(X_train)\n",
    "# custom_scaler.transform(X_test).head()\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     ('num_transformer', num_transformer, ['age', 'bmi']),\n",
    "#     ('children_scaler', CustomScaler(shrink_factor=3), ['children'])])\n",
    "\n",
    "# # Concatenate feature matrices outputs of different transformers\n",
    "# from sklearn.pipeline import FeatureUnion\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "# # Create a transformer that multiplies two columns\n",
    "# bmi_age_ratio_constructor = FunctionTransformer(lambda df: pd.DataFrame(df[\"bmi\"] / df[\"age\"]))\n",
    "# union = FeatureUnion([\n",
    "#     ('preprocess', preprocessor), # columns 0-8\n",
    "#     ('bmi_age_ratio', bmi_age_ratio_constructor) # new colums 9\n",
    "# ])\n",
    "# union.fit(X_train)\n",
    "# union\n",
    "\n",
    "#  Have the num and cat pipelines run in parallel\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_pipeline', num_pipeline, num_features),\n",
    "    ('cat_pipeline', cat_pipeline, cat_features)\n",
    "], remainder = 'drop')\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     ('num_transformer', num_transformer, make_column_selector(dtype_include=['int64'])),\n",
    "#     ('cat_transformer', cat_transformer, make_column_selector(dtype_include=['object']))],\n",
    "#     remainder='passthrough')\n",
    "# remainers are col that have not been selected- do not passthrough if you don't want them!\n",
    "preprocessor\n",
    "\n",
    "# Check preprocessing has worked\n",
    "# pd.DataFrame(preprocessor.fit_transform(X_train))\n",
    "\n",
    "# Make scorer if you want to use another method for scoring for methods\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, mean_squared_log_error\n",
    "# This is our metric to minimize\n",
    "rmsle = make_scorer(lambda y_true, y_pred: mean_squared_log_error(y_true, y_pred)**0.5)\n",
    "# This is our score to maximize\n",
    "rmsle_neg = make_scorer(lambda y_true, y_pred: -1 * mean_squared_log_error(y_true, y_pred)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048fdc3c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd59f2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You want to take the data preprocessing pipeline and add a model to this. You can keep adding different models and testing between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2215b07",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Add the model to the pipeline\n",
    "final_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "final_pipe\n",
    "\n",
    "# Check the final pipeline has worked and do scoring: Pass the fit final_pipe into a CV\n",
    "final_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Make y pred\n",
    "pre_test.predict(X_test)\n",
    "\n",
    "# Score pipeline model\n",
    "from sklearn.model_selection import cross_validate\n",
    "cv_score = cross_validate(\n",
    "    final_pipe,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv = 5,\n",
    "    scoring = rmsle) # made using make_scorer\n",
    "cv_score['test_score'].mean()\n",
    "\n",
    "# Check what params you can change\n",
    "final_pipe.get_params()\n",
    "\n",
    "# Hyperparams grid\n",
    "grid = {\n",
    "    'preprocessor__num_pipeline__n_imputer__strategy': ['mean', 'median'],\n",
    "    'model__C' : stats.uniform(0.1,100),\n",
    "    'model__l1_ratio': stats.uniform(0.1,0.9)\n",
    "}\n",
    "\n",
    "# Run RandomizedSearchCV\n",
    "rand_pipeline = RandomizedSearchCV(estimator=final_pipe,\n",
    "                   param_distributions=grid,\n",
    "                   cv=5,\n",
    "                   scoring='accuracy',\n",
    "                   n_iter=50,\n",
    "                   n_jobs=-1)\n",
    "rand_pipeline\n",
    "\n",
    "# # Run GridSearchCV\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# grid_search = GridSearchCV(\n",
    "#     final_pipe, \n",
    "#     param_grid={\n",
    "#         'preprocessing__num_transformer__imputer__strategy': ['mean', 'median'],\n",
    "#         'preprocessing__children_scaler__shrink_factor': [1,2,3,4],\n",
    "#         'linear_regression__alpha': [0.1, 0.5, 1, 5, 10]},\n",
    "#     cv=5,\n",
    "#     scoring=\"accuracy\")\n",
    "\n",
    "# Fit data to the randomised pipeline\n",
    "rand_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Check the best score and make predictions\n",
    "rand_pipeline.best_params_\n",
    "rand_pipeline.best_score_\n",
    "rand_pipeline.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2bb06d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Decision trees and ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9818be2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Decision tree\n",
    "\n",
    "# Instanciate and train model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=2)\n",
    "tree_clf.fit(X,y)\n",
    "\n",
    "# Make a prediction\n",
    "print(tree_clf.predict([[4,1]]))\n",
    "print(tree_clf.predict_proba([[4,1]]))\n",
    "# proba will only give you ratio of being in this cat in the selected leaf\n",
    "\n",
    "import graphviz\n",
    "# Export model graph\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree_clf, out_file=\"iris_tree.dot\", feature_names=X.columns,\n",
    "                class_names=['0','1','2'], rounded=True, filled=True)\n",
    "\n",
    "# Import model graph\n",
    "with open(\"iris_tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "    display(graphviz.Source(dot_graph))\n",
    "\n",
    "# Plot decision tree\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "plot_decision_regions(X.values, y.values, classifier=tree_clf)\n",
    "\n",
    "# Other way to plot decision tree/random forest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from ipywidgets import interact\n",
    "#@interact(max_depth=10)\n",
    "def plot_classifier(max_depth):\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    clf.fit(X_moon, y_moon)\n",
    "    plot_decision_regions(X_moon, y_moon, classifier=clf)\n",
    "plot_classifier(max_depth=10)\n",
    "\n",
    "# See r2 of different trees in a decision tree/random forest regressor model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "tree = DecisionTreeRegressor()\n",
    "# forest = RandomForestRegressor(n_estimators=100)\n",
    "cv_results = cross_validate(tree, X, y, scoring = \"r2\", cv=10)\n",
    "print(cv_results['test_score'])\n",
    "print('std: ', cv_results['test_score'].std())\n",
    "print('mean: ',cv_results['test_score'].mean())\n",
    "\n",
    "# Bagging any algorithm using BaggingRegressor/Classifier and working out r2\n",
    "from sklearn.ensemble import BaggingRegressor#, BaggingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression() # or classification model\n",
    "bagged_model = BaggingRegressor(linear_model, n_estimators=50)\n",
    "cv_results = cross_validate(bagged_model, X, y, scoring = \"r2\", cv=10)\n",
    "print('mean r2: ',cv_results['test_score'].mean())\n",
    "print('std r2: ', cv_results['test_score'].std())\n",
    "\n",
    "# Test bagged model on out-of-bag samples\n",
    "bagged_model = BaggingRegressor(\n",
    "    linear_model,\n",
    "    n_estimators=50,\n",
    "    oob_score=True\n",
    ")\n",
    "bagged_model.fit(X,y).oob_score_\n",
    "\n",
    "# Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest = RandomForestRegressor(n_estimators=100)\n",
    "cv_results = cross_validate(forest, X, y, scoring = \"r2\", cv=10)\n",
    "print(cv_results['test_score'])\n",
    "print('mean r2: ',cv_results['test_score'].mean())\n",
    "print('std r2: ', cv_results['test_score'].std())\n",
    "\n",
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#@interact(max_depth=5)\n",
    "def plot_classifier(max_depth):\n",
    "    cls = RandomForestClassifier(max_depth=max_depth)\n",
    "    cls.fit(X_moon, y_moon)\n",
    "    plot_decision_regions(X_moon, y_moon, classifier=cls)\n",
    "plot_classifier(max_depth=5)\n",
    "\n",
    "# AdaBoosted regressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "adaboost = AdaBoostRegressor(DecisionTreeRegressor(max_depth=3))\n",
    "cv_results = cross_validate(adaboost, X, y, scoring = \"r2\", cv=10)\n",
    "print('mean r2: ',cv_results['test_score'].mean())\n",
    "print('std r2: ', cv_results['test_score'].std())\n",
    "\n",
    "# AdaBoosted classifier and plot\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#@interact(n_estimators=[10, 30, 50,100], max_depth=3)\n",
    "def plot_classifier(n_estimators, max_depth):\n",
    "    model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=max_depth),\n",
    "                               n_estimators=n_estimators)    \n",
    "    model.fit(X_moon, y_moon)\n",
    "    plot_decision_regions(X_moon, y_moon, classifier=model)\n",
    "plot_classifier(n_estimators=50, max_depth=3)\n",
    "\n",
    "# Gradient boosting regressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor()\n",
    "xgb_reg.fit(\n",
    "    X_train, y_train,\n",
    "    # evaluate loss at each iteration\n",
    "    eval_set=[(X_val, y_val)], \n",
    "    # stop iterating when eval loss increases 5 times in a row\n",
    "    early_stopping_rounds=5\n",
    ") \n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "\n",
    "# Fit several base classifiers/regressors and votes on/averages the results\n",
    "from sklearn.ensemble import VotingClassifier#, VotingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "forest = RandomForestClassifier()\n",
    "logreg = LogisticRegression()\n",
    "ensemble = VotingClassifier(\n",
    "    estimators = [(\"rf\", forest),(\"lr\", logreg)],\n",
    "    voting = 'soft', # to use predict_proba of each classifier before voting\n",
    "    weights = [1,1] # to equally weight forest and logreg in the vote\n",
    ")\n",
    "ensemble.fit(X_moon, y_moon)\n",
    "plot_decision_regions(X_moon, y_moon, classifier=ensemble)\n",
    "\n",
    "# Train one estimator on the predictions of a previous one\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "ensemble = StackingClassifier(\n",
    "    estimators = [(\"rf\", RandomForestClassifier()),\n",
    "                  (\"knn\", KNeighborsClassifier(n_neighbors=10))],\n",
    "    final_estimator = LogisticRegression())\n",
    "ensemble.fit(X_moon, y_moon)\n",
    "plot_decision_regions(X_moon, y_moon, classifier=ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ad63a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Saving/Loading and using AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6515c946",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-e22afeaaa62d>, line 162)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-e22afeaaa62d>\"\u001b[0;36m, line \u001b[0;32m162\u001b[0m\n\u001b[0;31m    pip install deap update_checker tqdm stopit joblib torch\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Caching to avoid repeated computations\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "# Create a temp folder\n",
    "cachedir = mkdtemp()\n",
    "# Instantiate the pipeline with cache parameter\n",
    "pipe = Pipeline(steps, memory=cachedir)\n",
    "# Clear the cache directory after the cross-validation\n",
    "rmtree(cachedir)\n",
    "\n",
    "# Export/import final model/pipeline\n",
    "import pickle\n",
    "# Export pipeline as pickle file\n",
    "with open(\"pipeline.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tuned_pipe, file)\n",
    "# Load pipeline from pickle file\n",
    "my_pipeline = pickle.load(open(\"pipeline.pkl\",\"rb\"))\n",
    "my_pipeline.score(X_test, y_test)\n",
    "\n",
    "# AutoML- TPOT\n",
    "# install dependencies\n",
    "pip install deap update_checker tqdm stopit joblib torch\n",
    "# install xgboost optionally\n",
    "pip install xgboost\n",
    "# install tpot\n",
    "pip install tpot\n",
    "\n",
    "# Example code with iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tpot import TPOTClassifier\n",
    "# load iris dataset\n",
    "iris = load_iris()\n",
    "# hold out\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),\n",
    "    iris.target.astype(np.float64), train_size=0.75, test_size=0.25)\n",
    "# instanciate TPOTClassifier\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)\n",
    "# process autoML with TPOT\n",
    "tpot.fit(X_train, y_train)\n",
    "# print score\n",
    "print(tpot.score(X_test, y_test))\n",
    "# export TPOT pipeline to a python file\n",
    "tpot.export(os.path.join(os.getenv('HOME'),'code','tpot_iris_pipeline.py'))\n",
    "\n",
    "# Generated pipeline\n",
    "# ~/code/tpot_iris_pipeline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1)\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'], random_state=None)\n",
    "# Average CV score on the training set was: 0.9735177865612649\n",
    "exported_pipeline = make_pipeline(\n",
    "    Normalizer(norm=\"max\"),\n",
    "    KNeighborsClassifier(n_neighbors=5, p=1, weights=\"distance\")\n",
    ")\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288b636",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a729d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)  # This will only return the top 2 PC\n",
    "\n",
    "# Find all Principal Components\n",
    "pca.fit(X)\n",
    "\n",
    "# Print the PCs (as rows)\n",
    "# Expressed as linear combination of initial vector basis (13 columns)\n",
    "Wt = pd.DataFrame(pca.components_)\n",
    "Wt\n",
    "\n",
    "# Project dataset into the new PC basis: (eq. X.dot(Wt.T))\n",
    "Xp = pca.transform(X)\n",
    "pd.DataFrame(Xp)\n",
    "\n",
    "# Fit and transform\n",
    "Xp = pca2.fit_transform(X)\n",
    "pd.DataFrame(Xp2)\n",
    "\n",
    "# Work out the vif for PC (close to 1 as possible)\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "df = pd.DataFrame()\n",
    "df[\"vif_index\"] = [vif(Xp, i) for i in range(Xp.shape[1])]\n",
    "df\n",
    "\n",
    "# Display the variance ratios of PCs\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "# Plot the explained variance ratios- use elbow method to decide which to use\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component'); plt.ylabel('% explained variance');\n",
    "\n",
    "# Find principal components (not needed but fun to know)\n",
    "#eig_vals, eig_vecs = np.linalg.eig(np.dot(X.T,X))\n",
    "\n",
    "# Decompress/reconstruct features from pca (only if you kept all PCs)\n",
    "X_reconstructed = pca.inverse_transform(Xp)\n",
    "\n",
    "# Get the mean of all the instances of the dataset (mean value for each pixel across all images)\n",
    "pca.mean_\n",
    "\n",
    "# KMeans clustering\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=3)\n",
    "km.fit(Xp)\n",
    "\n",
    "# KMeans mini-batch clustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "km = MiniBatchKMeans(n_clusters=3,\n",
    "                    batch_size=6)\n",
    "km.fit(Xp)\n",
    "\n",
    "# Show co-ordinates of the centroids\n",
    "km.cluster_centers_\n",
    "\n",
    "# Show the classification of the rows\n",
    "km.labels_\n",
    "\n",
    "# predict (classify) a new X\n",
    "km.predict(new_X)\n",
    "    \n",
    "# Fit X and make y_pred in one command\n",
    "y_pred = k_model.fit_predict(X)\n",
    "\n",
    "# Let's observe the 2D slice of our dataset in the plan (PC1, PC2)\n",
    "plt.scatter(Xp[:,0], Xp[:,1], c=km.labels_)\n",
    "plt.title('KMeans clustering'); plt.xlabel('PC 1'); plt.ylabel('PC 2')\n",
    "\n",
    "# Choose K (num clusters) that minimize inertia (Kmeans().inertia_)- Use elbow method on graph\n",
    "inertias = []\n",
    "ks = range(1,10)\n",
    "for k in ks:\n",
    "    km_test = KMeans(n_clusters=k).fit(Xp)\n",
    "    inertias.append(km_test.inertia_)\n",
    "plt.plot(ks, inertias)\n",
    "plt.xlabel('k cluster number')\n",
    "    \n",
    "# Using hierarchical clustering to pick optimal K\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "plt.figure(figsize=(20, 10))\n",
    "dendrogram(linkage(X, method='ward'))\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "# Images\n",
    "# Show how many different colours in an img ([1] will just be 3)\n",
    "pd.DataFrame(img_vec).drop_duplicates().shape[0]\n",
    "\n",
    "# Reducing the number of colours to 32\n",
    "from sklearn.cluster import KMeans\n",
    "k_model = KMeans(n_clusters= 32)\n",
    "k_model.fit(img_vec)\n",
    "img_colors_32 = k_model.fit_predict(img_vec)\n",
    "# k_model.labels_  The predict is stored in the model\n",
    "    \n",
    "# Convert reduced image to a compressed image that can be shown\n",
    "X_compressed = k_model.cluster_centers_[k_model.labels_]\n",
    "X_compressed = X_compressed.astype('uint8') # convert to int between 0 and 255\n",
    "img_compressed = np.reshape(X_compressed, (512, 512, 3))\n",
    "    \n",
    "# Showing the image\n",
    "plt.imshow(img_compressed)\n",
    "\n",
    "# Showing images side by side\n",
    "fig, ax = plt.subplots(3, 2, figsize = (5,10))\n",
    "ax = ax.flatten()\n",
    "ax[0].imshow(pca_reshaped[0], cmap=plt.cm.gray)\n",
    "ax[1].imshow(pca_reshaped[1], cmap=plt.cm.gray)\n",
    "ax[2].imshow(pca_reshaped[2], cmap=plt.cm.gray)\n",
    "ax[3].imshow(pca_reshaped[3], cmap=plt.cm.gray)\n",
    "ax[4].imshow(pca_reshaped[4], cmap=plt.cm.gray)\n",
    "ax[5].imshow(pca_reshaped[-1], cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2a3f3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f7e9f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Time series\n",
    "! pip install --quiet statsmodels==0.11\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Make a test set of the last 40% of the data\n",
    "train_size = 0.6\n",
    "index = round(train_size*df.shape[0])\n",
    "df_train = df.iloc[:index]\n",
    "df_test = df.iloc[index+1:]\n",
    "\n",
    "# Plot acf for data or data_diff- determine q\n",
    "plot_acf(y, lags=50); # lags is how many data points will be shown\n",
    "\n",
    "# Plot pacf for data or data_diff- determine p\n",
    "plot_pacf(y, lags=50, color='r');\n",
    "\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "ar = np.r_[1, -arparams] # add zero-lag and negate (this is how ArmaProcess needs to be coded internally)\n",
    "ma = np.r_[1, maparams] # add zero-lag\n",
    "arma_process = ArmaProcess(ar, ma)\n",
    "\n",
    "# Check null hypothesis (data is non-stationary)\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "adfuller(y)[1]  # p-value\n",
    "\n",
    "# Create ARIMA model\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "arima = ARIMA(y, order=(p,d,q))  # if d= 0 then this is ARMA\n",
    "arima = arima.fit()\n",
    "arima.summary()\n",
    "\n",
    "# Use auto_arima to grid search values for p and q\n",
    "! pip install pmdarima --quiet\n",
    "import pmdarima as pm\n",
    "smodel = pm.auto_arima(y,\n",
    "                       start_p=1, max_p=2,\n",
    "                       start_q=1, max_q=2,\n",
    "                       seasonal=False,\n",
    "                       error_action='ignore',\n",
    "                       suppress_warnings=True,\n",
    "                       trace=True)\n",
    "model.summary() # assess the model\n",
    "\n",
    "# Differentiating the data\n",
    "y.diff().dropna()\n",
    "y.diff(12).dropna() # take the difference every 12 if montly data and seasonal\n",
    "\n",
    "# automatically estimate differencing term\n",
    "from pmdarima.arima.utils import ndiffs\n",
    "ndiffs(df['linearized'])\n",
    "\n",
    "# Make y_pred for the next 15 values. Manually plot too.\n",
    "y_forcast, y_SD, y_CI = arima.forecast(steps= 15)\n",
    "\n",
    "# Plot predictions from 85 to 100 without using y values for these if already known\n",
    "arima.plot_predict(start= 85, end = 100, dynamic= True)\n",
    "\n",
    "# Plot predictions on original values\n",
    "plt.plot(y[:85]);\n",
    "plt.plot(range(85,100), y_forcast)\n",
    "plt.plot(range(85,100), y_CI[:,0])\n",
    "plt.plot(range(85,100), y_CI[:,1])\n",
    "plt.fill_between(range(85,100), y_CI[:,0], y_CI[:,1], alpha= 0.1) #alpha is the transparancy\n",
    "\n",
    "# Plot the arima residuals and then the distribution\n",
    "residuals = pd.DataFrame(arima.resid)\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,3))\n",
    "residuals.plot(title=\"Residuals\", ax=ax[0])\n",
    "residuals.plot(kind='kde', title='Density', ax=ax[1]);\n",
    "\n",
    "# Check performance of ARIMA against true y\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import acf\n",
    "def forecast_accuracy(y_pred: pd.Series, y_true: pd.Series) -> float:\n",
    "    \n",
    "    mape = np.mean(np.abs(y_pred - y_true)/np.abs(y_true))  # Mean Absolute Percentage Error\n",
    "    me = np.mean(y_pred - y_true)             # ME\n",
    "    mae = np.mean(np.abs(y_pred - y_true))    # MAE\n",
    "    mpe = np.mean((y_pred - y_true)/y_true)   # MPE\n",
    "    rmse = np.mean((y_pred - y_true)**2)**.5  # RMSE\n",
    "    corr = np.corrcoef(y_pred, y_true)[0,1]   # Correlation between the Actual and the Forecast\n",
    "    mins = np.amin(np.hstack([y_pred.values.reshape(-1,1), y_true.values.reshape(-1,1)]), axis=1)\n",
    "    maxs = np.amax(np.hstack([y_pred.values.reshape(-1,1), y_true.values.reshape(-1,1)]), axis=1)\n",
    "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
    "    acf1 = acf(y_pred-y_true, fft=False)[1]                      # Lag 1 Autocorrelation of Error\n",
    "    return({'mape':mape, 'me':me, 'mae': mae, \n",
    "            'mpe': mpe, 'rmse':rmse, 'acf1':acf1, \n",
    "            'corr':corr, 'minmax':minmax})\n",
    "# Here is how to call the function and to reset the index\n",
    "forecast_accuracy(pd.Series(arima.forecast(steps= 15)[0]), y[-15:].reset_index(drop= True))\n",
    "\n",
    "# We define here a \"Plot forecast vs. real\", which also shows historical train set\n",
    "def plot_forecast(fc, train, test, upper=None, lower=None):\n",
    "    is_confidence_int = isinstance(upper, np.ndarray) and isinstance(lower, np.ndarray)\n",
    "    # Prepare plot series\n",
    "    fc_series = pd.Series(fc, index=test.index)\n",
    "    lower_series = pd.Series(upper, index=test.index) if is_confidence_int else None\n",
    "    upper_series = pd.Series(lower, index=test.index) if is_confidence_int else None\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10,4), dpi=100)\n",
    "    plt.plot(train, label='training', color='black')\n",
    "    plt.plot(test, label='actual', color='black', ls='--')\n",
    "    plt.plot(fc_series, label='forecast', color='orange')\n",
    "    if is_confidence_int:\n",
    "        plt.fill_between(lower_series.index, lower_series, upper_series, color='k', alpha=.15)\n",
    "    plt.title('Forecast vs Actuals')\n",
    "    plt.legend(loc='upper left', fontsize=8);\n",
    "\n",
    "# Using the plot function\n",
    "plot_forecast(forecast, train, test, confidence_int[:,0], confidence_int[:,1])\n",
    "\n",
    "# Now add back in the seasonal component (+/- exp if linerised)\n",
    "forecast_recons = np.exp(forecast)*result_mul.seasonal[150:]\n",
    "train_recons = np.exp(train)*result_mul.seasonal[0:150]\n",
    "test_recons = np.exp(test)*result_mul.seasonal[150:]\n",
    "lower_recons = np.exp(confidence_int)[:,0]*result_mul.seasonal[150:]\n",
    "upper_recons = np.exp(confidence_int)[:,1]*result_mul.seasonal[150:]\n",
    "# plt \n",
    "plot_forecast(forecast_recons, train_recons, test_recons, lower_recons.values, upper_recons.values)\n",
    "\n",
    "\n",
    "# Working with seasonal data\n",
    "\n",
    "\n",
    "# Compute Seasonal Index\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# additive seasonal component\n",
    "result_add = seasonal_decompose(df, model='additive')\n",
    "result_add.plot();\n",
    "\n",
    "# multiplacative seasonal component\n",
    "result_mul = seasonal_decompose(df, model='multiplacative')\n",
    "result_mul.plot();\n",
    "\n",
    "# Deseasonalizing the dataset\n",
    "df['deseasonalized'] = df.Sales.values/result_mul.seasonal\n",
    "\n",
    "# Split train test by year (dt data)\n",
    "df_train = df[df.index.year < 1970]\n",
    "df_test = df[df.index.year > 1969]\n",
    "\n",
    "# If monthly data plot these and see if seasonality is lost with differentiation of 12\n",
    "plt.plot(df_train['Sales'].diff().dropna())\n",
    "plt.plot(df_train['Sales'].diff(12).dropna())\n",
    "\n",
    "# Check the acf and pacf of this differentiation\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(df_train['Sales'].diff(12).dropna(), lags=24)\n",
    "plt.show()\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plot_pacf(df_train['Sales'].diff(12).dropna(), lags=24)\n",
    "plt.show()\n",
    "\n",
    "# Automatically look for the best hyperparams and use SARIMAX to create the model\n",
    "import pmdarima as pm\n",
    "model = pm.auto_arima(df_train['Sales'], \n",
    "                      start_P=0, start_Q=0,\n",
    "                      max_P=2, max_Q=2,\n",
    "                      D = 1, # if only one D\n",
    "                      m = 12, # seasonality\n",
    "                      seasonal = True,\n",
    "                      njobs=-1,\n",
    "                      trace=True,\n",
    "                      error_action='ignore',\n",
    "                      suppress_warnings=True)\n",
    "\n",
    "# Manually make SARIMAX model\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "sarima = SARIMAX(train, order=(0, 1, 1),seasonal_order=(1,0,2,12))\n",
    "sarima = sarima.fit()\n",
    "results = sarima.get_forecast(len(test), alpha=0.05)\n",
    "forecast = results.predicted_mean\n",
    "confidence_int = results.conf_int()\n",
    "# Reconstruct by taking exponential\n",
    "forecast_recons = pd.Series(np.exp(forecast), index=test.index)\n",
    "lower_recons = np.exp(confidence_int['lower log']).values\n",
    "upper_recons = np.exp(confidence_int['upper log']).values\n",
    "plot_forecast(forecast_recons, np.exp(train), np.exp(test), upper = upper_recons, lower=lower_recons)\n",
    "\n",
    "# Use forecast to plot predictions against reality\n",
    "n_periods = len(df_test)\n",
    "central, confint = model.predict(n_periods=n_periods, return_conf_int=True)\n",
    "index_y_test = df_test.index #pd.date_range(df_test.index[-1], periods = n_periods, freq='MS')\n",
    "# make series for plotting purpose\n",
    "central = pd.Series(central, index=index_y_test)\n",
    "lower = pd.Series(confint[:, 0], index=index_y_test)\n",
    "upper = pd.Series(confint[:, 1], index=index_y_test)\n",
    "# Plot\n",
    "plt.figure(figsize=(12,5), dpi=100)\n",
    "plt.plot(df_train, label = \"train values\")\n",
    "plt.plot(df_test, label = \"true test values\")\n",
    "plt.plot(central, color='darkgreen',label = \"forecast\")\n",
    "plt.fill_between(lower.index, \n",
    "                 lower, \n",
    "                 upper, \n",
    "                 color='k', alpha=.15)\n",
    "plt.title(\"SARIMA Forecast\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# With two correlated time series use one to predict the other\n",
    "# Using SARIMAX for endogenous and exogenous features\n",
    "SARIMAX(engod=df['electricity_price'],\n",
    "        exog=df['weather'],\n",
    "        order=(3, 0, 0),seasonal_order=(0,1,2,12)\n",
    "       )\n",
    "\n",
    "# Auto-fit the best SARIMAX with help from this exogenous time series\n",
    "import pmdarima as pm\n",
    "sarimax = pm.auto_arima(df[['value']], exogenous=df_augmented[['seasonal']],\n",
    "                           start_p=0, start_q=0,\n",
    "                           test='adf',\n",
    "                           max_p=2, max_q=2, m=12,\n",
    "                           start_P=0, seasonal=True,\n",
    "                           d=None, D=1, trace=True,\n",
    "                           error_action='ignore',  \n",
    "                           suppress_warnings=True, \n",
    "                           stepwise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cd267c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8eb17",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# when installing nltk for the first time we need to also download a few built in libraries\n",
    "# !pip install nltk   DO NOT WANT TO REINSTALL EACH TIME\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Text preprocessing\n",
    "\n",
    "\n",
    "# This function can clean portuguese text\n",
    "\n",
    "def clean(text):\n",
    "    # punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ')\n",
    "    \n",
    "    # lower case\n",
    "    lowercased = text.lower()\n",
    "    \n",
    "    # remove accents\n",
    "    no_accents = unidecode.unidecode(lowercased)\n",
    "    \n",
    "    # tokenize\n",
    "    tokenized = word_tokenize(no_accents)\n",
    "    \n",
    "    # remove numbers\n",
    "    words_only = [word for word in tokenized if word.isalpha()]\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    no_stopwords = [word for word in words_only if not word in stop_words]\n",
    "    \n",
    "    return ' '.join(no_stopwords)\n",
    "\n",
    "df.text = df.text.apply(clean)\n",
    "\n",
    "\n",
    "# Individual functions\n",
    "\n",
    "def lower_text(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "df.text = df.text.apply(lower_text)\n",
    "# Remove digits\n",
    "def remove_digits(text):\n",
    "    text = ' '.join(word for word in text if not word.isdigit())\n",
    "    return text\n",
    "df.text = df.text.apply(remove_digits)\n",
    "# Remove punctuation\n",
    "def remove_punct(text):\n",
    "    import string \n",
    "    for punctuation in string.punctuation:\n",
    "        text =  text.replace(punctuation, ' ')\n",
    "    return text\n",
    "df.text = df.text.apply(remove_punct)\n",
    "# Remove stop words and tokenize words in str\n",
    "def remove_stopwords(text):\n",
    "    from nltk.corpus import stopwords \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    text = [w for w in word_tokens if not w in stop_words] \n",
    "    return text\n",
    "df.text = df.text.apply(remove_stopwords)\n",
    "# Stemming words (use lemmatizer instead)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in text]\n",
    "# Lemmatizing words\n",
    "def lemmatizer_func(text):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return text\n",
    "df.text = df.text.apply(lemmatizer_func)\n",
    "\n",
    "# Rejoin (before vectorizing)\n",
    "def joiner_fun(text):\n",
    "    return ' '.join(text)\n",
    "clean_text.text = clean_text.text.apply(joiner_fun)\n",
    "\n",
    "# Vectorizing\n",
    "\n",
    "\n",
    "'''texts = ['i love football',\n",
    "         'football is a game i love',\n",
    "        'football football football']'''\n",
    "\n",
    "# Using bag-of-word representation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X.toarray()\n",
    "# Get the names of the features\n",
    "vectorizer.get_feature_names()\n",
    "# Make a df of the results\n",
    "pd.DataFrame(X.toarray(),columns = vectorizer.get_feature_names())\n",
    "\n",
    "# Using Tf-Idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "X = tf_idf_vectorizer.fit_transform(texts)\n",
    "X.toarray()\n",
    "pd.DataFrame(X.toarray(),columns = tf_idf_vectorizer.get_feature_names())\n",
    "\n",
    "# Params for above methods\n",
    "TfidfVectorizer(max_df = 0.8)\n",
    "TfidfVectorizer(min_df = 0.5)\n",
    "TfidfVectorizer(max_features = 2)\n",
    "TfidfVectorizer(ngram_range = (2,2))  # can increase acc over CountVector\n",
    "\n",
    "# Feature engineering\n",
    "\n",
    "\n",
    "# Give value 0 - 1 with 1 meaning no repeated words in obs\n",
    "def vocab_richness(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    total_length = len(tokens)\n",
    "    unique_words = set(tokens)\n",
    "    unique_word_length = len(unique_words)\n",
    "    return unique_word_length/total_length\n",
    "data['vocab richness'] = data.text.apply(vocab_richness)\n",
    "\n",
    "# Modelling\n",
    "\n",
    "\n",
    "# (Multinomial) Naive Bayes Algorithm for classification\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data.text)\n",
    "y = data.spam\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train,y_train)\n",
    "nb_model.score(X_test,y_test)\n",
    "# If not doing train/test then use model in CV\n",
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(nb_model, X, y, cv = 5, scoring= ['accuracy', 'f1'])\n",
    "\n",
    "# Model tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__ngram_range': ((1,1), (2,2)),\n",
    "    'nb__alpha': (0.1,1),}\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "grid_search.fit(data.text,y)\n",
    "grid_search.best_params_ # alpha is smoothing param\n",
    "grid_search.best_score_\n",
    "\n",
    "# Combining vectorized output and engineered features\n",
    "from sklearn.compose import ColumnTransformer\n",
    "column_trans = ColumnTransformer([('vec', CountVectorizer(), 'text')]\n",
    "                                 , remainder='passthrough')\n",
    "X_combined = column_trans.fit_transform(data[['text','vocab_richness']])\n",
    "\n",
    "# Using LDA (unsupervised learning)\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "vectorizer = TfidfVectorizer().fit(data['text'])\n",
    "data_vectorized = vectorizer.transform(data['text'])\n",
    "lda_model = LatentDirichletAllocation(n_components=2).fit(data_vectorized)\n",
    "# Print output from the model\n",
    "def print_topics(model, vectorizer):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-10 - 1:-1]])\n",
    "print_topics(lda_model, vectorizer)\n",
    "# Show proba of different classes for an observation\n",
    "example = [\"rice var congratulations save upenn\"]\n",
    "example_vectorized = vectorizer.transform(example)\n",
    "lda_vectors = lda_model.transform(example_vectorized)\n",
    "print(\"topic 0 :\", lda_vectors[0][0])\n",
    "print(\"topic 1 :\", lda_vectors[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d37aa",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e48122",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232f28e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pip install tensorflow\n",
    "from tensorflow.keras import *\n",
    "\n",
    "# If taget is an output that has more than 2 categories- make binary class matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_cat = to_categorical(y)\n",
    "\n",
    "# Using a large dataset sample a proportion of the dataset\n",
    "reduction_factor = 10 # Sample 10% of dataset\n",
    "# Choose (index length/ reduction factor) number of rand indexes in length\n",
    "idx_train =  np.random.choice(len(images_train), round(len(images_train)/reduction_factor))\n",
    "idx_test =  np.random.choice(len(images_test), round(len(images_test)/reduction_factor))\n",
    "# Use these indexes to choose obs\n",
    "images_train_small = images_train[idx_train]\n",
    "images_test_small = images_test[idx_test]\n",
    "labels_train_small = labels_train[idx_train]\n",
    "labels_test_small = labels_test[idx_test]\n",
    "# Inspect sets\n",
    "print(images_train.shape, images_test.shape)\n",
    "unique, counts = np.unique(labels_train_small, return_counts=True) # Returns label and count of this label\n",
    "dict(zip(unique, counts)) # Returns dict of label with count\n",
    "\n",
    "# Workflow\n",
    "\n",
    "# 1. Declaration of the model and its architecture : Defining fθ0\n",
    "from tensorflow.keras import regularizers, models, layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "normalizer = Normalization() # Normalizes to a normal curve of 1 mean and 0 SD\n",
    "normalizer.adapt(X_train) # \"Fit\" it on the train set or add to model\n",
    "X_train_norm = normalizer(X_train) # Use normalizer as a function to transform data\n",
    "print(normalizer(X_train).numpy().std()) # Can do analysis\n",
    "# set regularizers (only regularize later layers)\n",
    "reg_l1 = regularizers.L1(0.01)\n",
    "reg_l2 = regularizers.L2(0.01)\n",
    "reg_l1_l2 = regularizers.l1_l2(l1=0.005, l2=0.0005)\n",
    "# Instantiate model\n",
    "model = models.Sequential() # this model will work sequentially down the layers\n",
    "# Add normalizer to the model\n",
    "model.add(normalizer)\n",
    "# First layer : 10 neurons and ReLU as activation function\n",
    "model.add(layers.Dense(10, input_dim=4, activation='relu')) # if 4 features\n",
    "# Adding further layers (no longer need to define input_dim)\n",
    "model.add(layers.Dense(10, activation='relu', kernel_regularizer=reg_l1))\n",
    "model.add(layers.Dense(10, activation='relu', kernel_regularizer=reg_l2))\n",
    "model.add(layers.Dense(10, activation='relu', kernel_regularizer=reg_l1_l2))\n",
    "# Set proportion of nodes to be temp dropped each learning cycle\n",
    "model.add(layers.Dropout(rate=0.2))\n",
    "# Regression task needs linear activation function\n",
    "model.add(layers.Dense(1, activation='linear')) # This will pred 1 value\n",
    "# Classification needs softmax (multiple classes) or sigmoid (binary)\n",
    "model.add(layers.Dense(8, activation='softmax')) # 8 classes\n",
    "model.add(layers.Dense(1, activation='sigmoid')) # binary classes\n",
    "\n",
    "# 2. Definition of the methods to learn θ: Saying how to go from θ0 to θ\n",
    "# REGRESSION\n",
    "model.compile(loss='mse', \n",
    "              optimizer='adam', \n",
    "              metrics=['mae','mse'])\n",
    "# CLASSIFICATION WITH 2 CLASSES\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', 'Recall', 'Precision'])\n",
    "# CLASSIFICATION WITH N (let's say 14) CLASSES\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy', 'Recall', 'Precision'])\n",
    "# Can also specify metrics and loss in tf\n",
    "accuracy = tf.keras.metrics.Accuracy() # accept fine-tuning arguments\n",
    "loss = tf.keras.losses.MSE() # accept finetuning arguments\n",
    "model.compile(loss=loss, metric=accuracy)\n",
    "\n",
    "\n",
    "# 3. Fitting on on data: Going from θ0 to θ based on the data X and y\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=20,\n",
    "                   monitor='val_loss',  # can change to other like val_recall if detecting fraud transactions\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=1000,\n",
    "                    batch_size=16,\n",
    "                    verbose=0,\n",
    "                    callbacks=[es])\n",
    "# If you have not set a val set then do this in fit\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.3,\n",
    "                    epochs=1000,\n",
    "                    batch_size=16,\n",
    "                    verbose=0,\n",
    "                    callbacks=[es],\n",
    "                    shuffle=True)\n",
    "# LAST 30% of train indexes used as validation if you do not set shuffle!\n",
    "\n",
    "# 4. Making predictions\n",
    "y_new = model.predict(X_new)\n",
    "\n",
    "\n",
    "\n",
    "# Practical method\n",
    "\n",
    "def initialize_model():\n",
    "    ### YOUR MODEL ARCHITECTURE HERE\n",
    "    # 1. Declare the model and build the architecture\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(25, input_dim=162, activation='relu'))\n",
    "    model.add(layers.Dense(15, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    return model\n",
    "\n",
    "def compile_model(model, optimizer_name):\n",
    "    # 2. Define the methods to learn θ\n",
    "    model.compile(loss='mse', \n",
    "              optimizer=optimizer_name, \n",
    "              metrics=['mae','mse']) # can add mse too\n",
    "    return model\n",
    "\n",
    "model = initialize_model()\n",
    "model = compile_model(model, 'adam')\n",
    "\n",
    "# Use the instantiated model fit to look at the history of your model optimization\n",
    "history.__dict__ # look at loss and metrics per epoch\n",
    "\n",
    "# Plot val and training model loss and metrics\n",
    "def plot_loss_accuracy(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    \n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    return plt.show()\n",
    "\n",
    "def plot_loss_mae(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,4))\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylim(ymin=0, ymax=30)\n",
    "    ax1.legend(['Train', 'Validation'], loc='best')\n",
    "    \n",
    "    ax2.plot(history.history['mae'])\n",
    "    ax2.plot(history.history['val_mae'])\n",
    "    ax2.set_title('MAE')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylim(ymin=0, ymax=20)\n",
    "    ax2.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_mse(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,4))\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylim(ymin=0, ymax=20)\n",
    "    ax1.legend(['Train', 'Validation'], loc='best')\n",
    "    \n",
    "    ax2.plot(history.history['mse'])\n",
    "    ax2.plot(history.history['val_mse'])\n",
    "    ax2.set_title('MSE')\n",
    "    ax2.set_ylabel('MSE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylim(ymin=0, ymax=30)\n",
    "    ax2.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_prc(name, labels, predictions, **kwargs):\n",
    "    precision, recall, _ = precision_recall_curve(labels, predictions)\n",
    "    plt.plot(precision, recall, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plot_prc(\"Test\", y_test, y_pred_proba, linestyle='--')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# use Keras metric objects for fine-tuning\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.Recall(),\n",
    "    keras.metrics.Precision(),\n",
    "    keras.metrics.AUC(name='prc', curve='PR'),  # precision-recall curve\n",
    "    keras.metrics.AUC(\n",
    "    num_thresholds = 200,\n",
    "    curve='ROC')\n",
    "]\n",
    "\n",
    "model.compile(metric=metric)\n",
    "\n",
    "# Custom metrics\n",
    "def custom_mse(y_true, y_pred):\n",
    "    squared_diff = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_mean(squared_diff)\n",
    "model.compile(metrics=[custom_mse])\n",
    "\n",
    "# Optimizing hyper-params (default then change learning rate)\n",
    "opt = tensorflow.keras.optimizers.Adam(\n",
    "    learning_rate=0.01, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# Get an output of shape and params for each layer\n",
    "model.summary()\n",
    "# Evaluate performance (returns loss and metrics)\n",
    "model.evaluate(scaler.transform(X_test), y_test, verbose=0)\n",
    "# Use the instantiated model fit to look at the \n",
    "\n",
    "# Plot decision regions from your model\n",
    "from utils.plots import plot_decision_regions\n",
    "plot_decision_regions(X,y, model)\n",
    "\n",
    "\n",
    "# Saving and loading models\n",
    "from tensorflow.keras import models\n",
    "# Let's say that you have a `model`\n",
    "# You can save it :\n",
    "models.save_model(model, 'my_model')\n",
    "# and you can load it somewhere else :\n",
    "loaded_model = models.load_model('my_model')\n",
    "\n",
    "# Sampling strategies\n",
    "# pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Sampling strategies\n",
    "oversample = SMOTE(sampling_strategy=0.5) # float sampling is only for binary (ratio between majority and minority)\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "# Pipelining the two strategies\n",
    "steps =  [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# Rebalance the dataset\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "# Reshuffle the resampled data just to be sure\n",
    "stacked_resampled_data = np.hstack([X_train_resampled, y_train_resampled.reshape(-1,1)])\n",
    "np.random.shuffle(stacked_resampled_data)\n",
    "X_train_resampled = stacked_resampled_data[:,:30]\n",
    "y_train_resampled = stacked_resampled_data[:,-1]\n",
    "'''The minority category will now be around 30% of the total as you are adding the resampled data\n",
    "onto the orginial dataset and the original data set was virtually all non-fraud'''\n",
    "\n",
    "\n",
    "\n",
    "# Wrapping NN into SKlearn (allows you to make pipelines- useful for gridsearch)\n",
    "# 1. Create model\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "def create_model():\n",
    "    normalizer = Normalization() # Instantiate a \"normalizer\" layer\n",
    "    normalizer.adapt(X_train_num) # \"Fit\" it on the train set\n",
    "    # Define architecture without input shape yet, as we don't know the shape post preprocessing\n",
    "    model = Sequential()\n",
    "    model.add(normalizer) # Use it as first sequential step\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(15, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 2. Make the model wrapper\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # or KerasRegressor\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                        epochs=10, \n",
    "                        batch_size=32, \n",
    "                        verbose=0)\n",
    "\n",
    "# 3. Wrap model into pipeline and evaluate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(preproc, model)\n",
    "cross_val_score(pipe, X_train, y_train, cv=3)\n",
    "\n",
    "# GridSearchCV\n",
    "# 4. Define the GridSearchCV model and wrap into pipeline\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "def create_model_grid(activation = 'relu', \n",
    "                      optimizer='rmsprop'):\n",
    "    normalizer = Normalization() # Instantiate a \"normalizer\" layer\n",
    "    normalizer.adapt(X_train_num) # \"Fit\" it on the train set\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(normalizer) # Use it as first sequential step\n",
    "    model.add(layers.Dense(32, activation=activation))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(15, activation=activation))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_grid = KerasClassifier(\n",
    "    build_fn=create_model_grid,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "pipe_grid = make_pipeline(preproc, model_grid)\n",
    "#pipe_grid.get_params()\n",
    "\n",
    "# 5. Define the GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = dict(\n",
    "    columntransformer__standardscaler__with_mean=[True, False], # Preprocessing hyperparams\n",
    "    kerasclassifier__activation=['tanh', 'relu'], # Architecture hyperparams\n",
    "    kerasclassifier__optimizer=[\"adam\", \"rmsprop\"], # Compiler hyperparams\n",
    "    kerasclassifier__batch_size=[8, 64], # Fit hyperparams\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(estimator=pipe_grid,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=2,\n",
    "                    verbose=2,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "#6. Fit the model and return the best results\n",
    "grid.fit(X_train, y_train);\n",
    "grid.best_params_\n",
    "grid.best_score_\n",
    "cross_val_score(grid.best_estimator_, X_train, y_train)\n",
    "\n",
    "# Loading large datasets through batching (see optimizer, loss and fitting recap)\n",
    "#! pip install -U --quiet tensorflow_datasets Pillow\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "# download 229Mo of images\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file(origin=dataset_url, \n",
    "                                   fname='flower_photos', \n",
    "                                   untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "# Prepare to load images in RAM memory batch per batch\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  batch_size=32)\n",
    "# Auto categories data based on folder structure\n",
    "# Fit the dataset by iterating over the images (loading them into RAM in batches)\n",
    "model.fit(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392bd05",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5195f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use reshape to flatten image prior to last steps\n",
    "from tensorflow.keras.layers import Reshape\n",
    "model = Sequential()\n",
    "model.add(Reshape((5*5*1,), input_shape=(5,5,1))) # will change to vector of 25\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# Flattenning of a 225*225 image with 3 colors\n",
    "model.add(Reshape((225*225*3,), input_shape=(225,225,3))) # only put input_shape if start\n",
    "# Or just use .Flatten!\n",
    "model.add(layers.Flatten(input_shape=(225, 225, 3)))\n",
    "\n",
    "# Normalising imaging data\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# Adding kernels\n",
    "# Will make 6 filters of 3 kernels in each (as input shape is 3D)\n",
    "model.add(layers.Conv2D(6, kernel_size=(3, 3), activation='relu', input_shape=(225, 225, 3)))\n",
    "model.add(layers.Conv2D(4, kernel_size=(3), activation='relu')) # kernel_size = 3 <==> (3, 3)\n",
    "# Adding kernel that will take strides of 2 col by 2 rows\n",
    "model.add(layers.Conv2D(16, (2,2), strides=(2,2), input_shape=(225, 225, 3), activation=\"relu\"))\n",
    "\n",
    "# Padding='valid' : no-padding, the output is smaller than the input\n",
    "model = Sequential()\n",
    "model.add(layers.Conv2D(16, (2,2), input_shape=(225, 225, 3), padding='valid', activation=\"relu\"))\n",
    "model.summary()\n",
    "# Padding='same' : padded with enough empty pixels to get an output of the same size as the input\n",
    "model = Sequential()\n",
    "model.add(layers.Conv2D(16, (2,2), input_shape=(225, 225, 3), padding='same', activation=\"relu\"))\n",
    "model.summary()\n",
    "\n",
    "# Good to add a pool layer after each convolution\n",
    "model = Sequential()\n",
    "model.add(layers.Conv2D(16, kernel_size=(2,2), strides=(2,2), input_shape=(225, 225, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2))) # take max value within a 2x2 matrix\n",
    "model.add(layers.Conv2D(16, kernel_size=(2,2)))\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2), activation=\"relu\")) # take the av value within a 2x2 matrix\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1, activation='sigmoid')) \n",
    "model.summary()\n",
    "\n",
    "# Reshape the X to explicitely add a single \"color\" channel\n",
    "X_train = X_train.reshape(len(X_train), 28, 28, 1) # greyscale\n",
    "X_test = X_test.reshape(len(X_test), 28, 28, 1) # make 3 if color image\n",
    "# Or use expand_dims\n",
    "from tensorflow.keras.backend import expand_dims\n",
    "X_train = expand_dims(X_train)\n",
    "X_test = expand_dims(X_train)\n",
    "\n",
    "# One Hot Encode our Target for TensorFlow processing\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_cat_train = to_categorical(y_train, num_classes=10) # handwritten numbers\n",
    "y_cat_test = to_categorical(y_test, num_classes=10) # 0-9\n",
    "\n",
    "# ——— CNN example\n",
    "model = Sequential()\n",
    "model.add(layers.Conv2D(16, (3,3), input_shape=(28, 28, 1), padding='same', activation=\"relu\"))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "model.add(layers.Conv2D(32, (2,2), padding='same', activation=\"relu\"))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2))) \n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(50, activation='relu')) # intermediate layer\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "# ——— CNN function\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "def initialize_model():\n",
    "    model = models.Sequential()\n",
    "    ### First convolution & max-pooling\n",
    "    model.add(layers.Conv2D(8, (4), input_shape=(28, 28, 1),\n",
    "                            padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    ### Second convolution & max-pooling\n",
    "    model.add(layers.Conv2D(16, (3), activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "    ### One fully connected\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    ### Last layer (let's say a classification with 10 output)\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    ### Model compilation\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy', 'Recall', 'Precision'])\n",
    "    return model\n",
    "\n",
    "model = initialize_model()\n",
    "model.summary()\n",
    "visualkeras.layered_view(model, legend=True)\n",
    "\n",
    "es = EarlyStopping(patience=2,\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train_cat,\n",
    "                    validation_split=0.3,\n",
    "                    epochs=5,\n",
    "                    batch_size=16,\n",
    "                    verbose=0,\n",
    "                    callbacks=[es],\n",
    "                    shuffle=True)\n",
    "\n",
    "# Visualize pipeline\n",
    "#pip install visualkeras\n",
    "import visualkeras\n",
    "visualkeras.layered_view(model, legend=True)\n",
    "\n",
    "# Access first convolution layer\n",
    "layer_1 = model.layers[0]\n",
    "# Access this layers 16 kernels. Let's print the last one:\n",
    "plt.imshow(layer_1.weights[0][:,:,:,15], cmap='gray');\n",
    "# Compute the output of the first layer (called it's activation)\n",
    "# By calling it with a \"batch\" of images (let's take 10)\n",
    "batch = X_train[0:10]\n",
    "activation_1 = layer_1(batch)\n",
    "\n",
    "# Display all 16 channels outputs of the first layer, applied to the first image only\n",
    "fig, axs = plt.subplots(4,4, figsize=(15,6))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        feature_map = activation_1[0,:,:,i+j]\n",
    "        axs[i,j].imshow(feature_map, cmap='gray')\n",
    "\n",
    "        \n",
    "# Get kernels from CNN\n",
    "def get_kernel(layer_number, filter_number, channel_number):\n",
    "\n",
    "    weight_or_bias = 0\n",
    "    k = model.layers[layer_number].weights[0].numpy()[:,\n",
    "                                                      :,\n",
    "                                                      channel_number,\n",
    "                                                      filter_number]\n",
    "\n",
    "    return k\n",
    "\n",
    "# Plot input image against Kernel against output image\n",
    "\n",
    "kernel_1 = [\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 0],\n",
    "    [-1, -1, -1]\n",
    "]\n",
    "def plot_convolution(img, kernel, activation=False):\n",
    "    ''' The following printing function ease the visualization'''\n",
    "    img = np.squeeze(img)\n",
    "    output_img = compute_convolution(img, kernel)\n",
    "    if activation:\n",
    "        output_img = np.maximum(output_img, 0)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax1 = plt.subplot2grid((3,3),(0,0), rowspan=3)\n",
    "    ax1.imshow(img, cmap='gray')\n",
    "    ax1.title.set_text('Input image')\n",
    "    ax2 = plt.subplot2grid((3,3),(1, 1))\n",
    "    ax2.imshow(kernel, cmap='gray')\n",
    "    ax2.title.set_text('Kernel')    \n",
    "    ax3 = plt.subplot2grid((3,3),(0, 2), rowspan=3)\n",
    "    ax3.imshow(output_img, cmap='gray')\n",
    "    ax3.title.set_text('Output image')    \n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "# Plot the images and kernel\n",
    "plot_convolution(X[0], kernel_1, activation=False)\n",
    "\n",
    "\n",
    "# Get the activation ('Output image') of different layers\n",
    "\n",
    "# List all 9 layers\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "# Instanciate 9 sub-models: [layer1, layer1-->layer2, layer1-->layer2-->layer3, ...]\n",
    "# Reusing already trained weights and biases\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "# Compute the 9 output of each sub-models\n",
    "activations = activation_model.predict(X) \n",
    "def get_activation(activations, image_number, layer_number, filter_number):\n",
    "    '''return activation map for a given layer, image, and filter number'''\n",
    "    return activations[layer_number][image_number][:, :, filter_number]\n",
    "\n",
    "# Getting the activations\n",
    "get_activation(activations, 0, 2, 3) # Example\n",
    "\n",
    "\n",
    "# Data augmentation\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=(0.8, 1.2),\n",
    "    )\n",
    "\n",
    "datagen.fit(X_train)\n",
    "X_augmented_iterator = datagen.flow(X_train, shuffle=False, batch_size=1) # Generates batches of augmented data\n",
    "# X_augmented_iterator[0] is the first augmented image\n",
    "\n",
    "# Visualize augmented data\n",
    "for i, (raw_image, augmented_image) in enumerate(zip(X_train, X_augmented_iterator)):\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 2))\n",
    "    ax1.imshow(raw_image)\n",
    "    ax2.imshow(augmented_image[0])\n",
    "    plt.show()\n",
    "    \n",
    "    if i > 10:\n",
    "        break\n",
    "        \n",
    "# Transfer learning\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 # from VGG16 model\n",
    "\n",
    "def load_model():\n",
    "    model = VGG16(weights=\"imagenet\", include_top=False, input_shape=X_train[0].shape)\n",
    "    return model\n",
    "\n",
    "def set_nontrainable_layers(model):\n",
    "    # Set the first layers to be untrainable\n",
    "    model.trainable = False\n",
    "    return model\n",
    "\n",
    "# General flow for transfer learning\n",
    "base_model = load_model()\n",
    "\n",
    "def add_last_layers(model):\n",
    "    '''Take a pre-trained model, set its parameters as non-trainables, and add additional trainable layers on top'''\n",
    "    base_model = set_nontrainable_layers(model)\n",
    "    flatten_layer = layers.Flatten()\n",
    "    dense_layer = layers.Dense(500, activation='relu')\n",
    "    prediction_layer = layers.Dense(3, activation='softmax')\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        flatten_layer,\n",
    "        dense_layer,\n",
    "        prediction_layer\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "def build_model():   \n",
    "    model = load_model()\n",
    "    model = add_last_layers(model)\n",
    "    \n",
    "    opt = optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model.summary() # this will show you how many trainable and non-trainable params there are\n",
    "# Not fit and do other steps to the model\n",
    "\n",
    "\n",
    "# Autoencoders\n",
    "\n",
    "# Encoding data\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "def build_encoder(latent_dimension):\n",
    "    '''returns an encoder model, of output_shape equals to latent_dimension'''\n",
    "    encoder = Sequential()\n",
    "    encoder.add(Conv2D(8, (2,2), input_shape=(28, 28, 1), activation='relu'))\n",
    "    encoder.add(MaxPooling2D(2))\n",
    "    encoder.add(Conv2D(16, (2, 2), activation='relu'))\n",
    "    encoder.add(MaxPooling2D(2))\n",
    "    encoder.add(Conv2D(32, (2, 2), activation='relu'))\n",
    "    encoder.add(MaxPooling2D(2))     \n",
    "    encoder.add(Flatten())\n",
    "    encoder.add(Dense(latent_dimension, activation='tanh'))\n",
    "    return encoder\n",
    "\n",
    "# Decoding data (using Conv2DTranspose that does the opposite of a convolution)\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "\n",
    "def build_decoder(latent_dimension):\n",
    "    # You need to get back to (28,28,1) so do steps accordingly\n",
    "    decoder = Sequential() \n",
    "    decoder.add(Dense(7*7*8, activation='tanh', input_shape=(latent_dimension,))) # Makes vector 7*7*8 for reshaping\n",
    "    decoder.add(Reshape((7, 7, 8)))  # no batch axis here\n",
    "    # Conv2DTranspose with stride 2 will double the shape\n",
    "    decoder.add(Conv2DTranspose(8, (2, 2), strides=2, padding='same', activation='relu')) # Double shape to (14,14,1)\n",
    "    decoder.add(Conv2DTranspose(1, (2, 2), strides=2, padding='same', activation='relu'))# Double shape to (28,28,1)\n",
    "    return decoder\n",
    "\n",
    "# Concatenate encoder and decoder\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "def build_autoencoder(encoder, decoder):\n",
    "    inp = Input((28, 28,1)) # input and output shape\n",
    "    encoded = encoder(inp)\n",
    "    decoded = decoder(encoded)\n",
    "    autoencoder = Model(inp, decoded) # Concatenate using functionalAPI in Model Class\n",
    "    return autoencoder\n",
    "\n",
    "autoencoder = build_autoencoder(encoder, decoder)\n",
    "\n",
    "# Compile the autoencoder\n",
    "def compile_autoencoder(autoencoder):\n",
    "    autoencoder.compile(loss='mse', # Use mse for pixel-by-pixel error minimization\n",
    "                  optimizer='adam')\n",
    "    return autoencoder\n",
    "\n",
    "compile_autoencoder(autoencoder)\n",
    "\n",
    "# Fit the data to the autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=20, batch_size=32)\n",
    "\n",
    "# Generate and display reconstructed images\n",
    "prediction = autoencoder.predict(X_train, verbose=0, batch_size=100)\n",
    "plt.imshow(prediction[0].reshape(28,28), cmap='Greys')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Can encode your data using the encoder (will be represented by the latent_dimension (2d in this case))\n",
    "X_encoded = encoder.predict(X_train, verbose=1)\n",
    "\n",
    "\n",
    "# Can denoise noisy data\n",
    "encoder = build_encoder(2)\n",
    "decoder = build_decoder(2)\n",
    "autoencoder = build_autoencoder(encoder, decoder)\n",
    "compile_autoencoder(autoencoder)\n",
    "history_denoising = autoencoder.fit(X_train_noisy, X_train, epochs=20, batch_size=32)\n",
    "prediction = autoencoder.predict(X_test_noisy, verbose=1)\n",
    "# Show the denoised images\n",
    "plt.imshow(prediction[0].reshape(28,28), cmap='Greys')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb0d90",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Random functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2d3f5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_convolution(img, kernel):\n",
    "    # Parameters\n",
    "    kernel = np.array(kernel)\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "    img = np.squeeze(img) # Removes dimensions of size 1\n",
    "    img_height, img_width = img.shape\n",
    "    \n",
    "    array = []\n",
    "\n",
    "    for x in range(img_height-kernel_height):\n",
    "        arr = []\n",
    "        \n",
    "        for y in range(img_width - kernel_width):\n",
    "            \n",
    "            a = np.multiply(img[x:x+kernel_height, y:y+kernel_width], kernel)\n",
    "            arr.append(a.sum())\n",
    "            \n",
    "        array.append(arr)\n",
    "        \n",
    "    return array\n",
    "\n",
    "\n",
    "kernel_1 = [\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 0],\n",
    "    [-1, -1, -1]\n",
    "]\n",
    "\n",
    "def plot_convolution(img, kernel, activation=False):\n",
    "    ''' The following printing function ease the visualization'''\n",
    "    \n",
    "    img = np.squeeze(img)\n",
    "    output_img = compute_convolution(img, kernel)\n",
    "    if activation:\n",
    "        output_img = np.maximum(output_img, 0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    ax1 = plt.subplot2grid((3,3),(0,0), rowspan=3)\n",
    "    ax1.imshow(img, cmap='gray')\n",
    "    ax1.title.set_text('Input image')\n",
    "    \n",
    "    ax2 = plt.subplot2grid((3,3),(1, 1))\n",
    "    ax2.imshow(kernel, cmap='gray')\n",
    "    ax2.title.set_text('Kernel')    \n",
    "    \n",
    "    ax3 = plt.subplot2grid((3,3),(0, 2), rowspan=3)\n",
    "    ax3.imshow(output_img, cmap='gray')\n",
    "    ax3.title.set_text('Output image')    \n",
    "\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "def get_kernel(layer_number, filter_number, channel_number):\n",
    "\n",
    "    weight_or_bias = 0\n",
    "    k = model.layers[layer_number].weights[0].numpy()[:,\n",
    "                                                      :,\n",
    "                                                      channel_number,\n",
    "                                                      filter_number]\n",
    "\n",
    "    return k\n",
    "\n",
    "def get_activation(activations, image_number, layer_number, filter_number):\n",
    "    '''return activation map for a given layer, image, and filter number'''\n",
    "    return activations[layer_number][image_number][:, :, filter_number]\n",
    "\n",
    "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
    "    if axs is not None:\n",
    "        ax1, ax2 = axs\n",
    "    else:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
    "        exp_name = '_' + exp_name\n",
    "    ax1.plot(history.history['loss'], label='train' + exp_name)\n",
    "    ax1.plot(history.history['val_loss'], label='val' + exp_name)\n",
    "    ax1.set_ylim(0., 2.2)\n",
    "    ax1.set_title('loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
    "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
    "    ax2.set_ylim(0.25, 1.)\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "    return (ax1, ax2)\n",
    "\n",
    "def draw_triangle():\n",
    "    dx = np.random.uniform(0.1, 0.3)\n",
    "    dy = np.random.uniform(0.1, 0.3)\n",
    "    noise_x = np.random.uniform(0.0, 0.1)\n",
    "    noise_y = np.random.uniform(0.0, 0.1)    \n",
    "    \n",
    "    x = np.random.uniform(0, 1-dx-noise_x)\n",
    "    y = np.random.uniform(0, 1-dy)\n",
    "    X = np.array([[x,y], [x+dx+noise_x,y], [x+dx/2, y+dy+noise_y]])\n",
    "\n",
    "    t1 = plt.Polygon(X, color='black')\n",
    "    plt.gca().add_patch(t1)\n",
    "    \n",
    "def draw_circle():\n",
    "    r = np.random.uniform(0.1, 0.25)\n",
    "    x = np.random.uniform(0+r, 1-r)\n",
    "    y = np.random.uniform(0+r, 1-r)\n",
    "\n",
    "    circle1 = plt.Circle((x, y), r, color='black')\n",
    "    plt.gcf().gca().add_artist(circle1)\n",
    "    \n",
    "def create_image(form, path):\n",
    "    plt.figure(figsize=(1, 1))\n",
    "    if form == 'circle':\n",
    "        draw_circle()\n",
    "    elif form == 'triangle':\n",
    "        draw_triangle()\n",
    "    plt.axis('off')\n",
    "    plt.savefig(path, dpi=80, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def create_images(path):\n",
    "    nb_circles = 100\n",
    "    nb_triangles = 100\n",
    "    \n",
    "    for i in range(nb_circles):\n",
    "        c_path = os.path.join(path, 'circles', f'circle_{i}.png')\n",
    "        create_image('circle', c_path)\n",
    "        \n",
    "    for i in range(nb_triangles):\n",
    "        t_path = os.path.join(path, 'triangles', f'triangle_{i}.png')\n",
    "        create_image('triangle', t_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b45b69f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb410d7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Making X and y\n",
    "\n",
    "X = np.array([sequence_a, sequence_b, sequence_c]).astype(np.float32) # make sure is float\n",
    "y = np.array([y_a, y_b, y_c]).astype(np.float32)\n",
    "X.shape # (n_SEQUENCES, n_OBSERVATIONS, n_FEATURES)\n",
    "y.shape # (n_SEQUENCES, n output per sequence)\n",
    "\n",
    "# RNN pipeline\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "model = Sequential()\n",
    "# return_sequences outputs y at each time step (default returns only last y)- needed if using multiple RNN layers\n",
    "model.add(layers.SimpleRNN(units=2, return_sequences=True, activation='tanh', input_shape=(4,3))) # obs and feat\n",
    "model.add(layers.SimpleRNN(3, return_sequences=False))\n",
    "model.add(layers.Dense(1, activation=\"linear\"))\n",
    "# The compilation\n",
    "model.compile(loss='mse', \n",
    "              optimizer='rmsprop')  # Recommanded optimizer for RNN\n",
    "# The fit\n",
    "model.fit(X, y,\n",
    "         batch_size=16,\n",
    "         epochs=10, verbose=0)\n",
    "# The prediction (one per sequence/city)\n",
    "model.predict(X)\n",
    "\n",
    "# Using other layers\n",
    "\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU\n",
    "###  Simple RNN  ###\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=10, activation='tanh'))  # tanh is default param\n",
    "###  LSTM   ###\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=10, activation='tanh'))\n",
    "###  GRU ###\n",
    "model = Sequential()\n",
    "model.add(GRU(units=10, activation='tanh'))\n",
    "\n",
    "# Add padding (if missing values)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_pad = pad_sequences(X, dtype='float32') # int32 by default\n",
    "# As beggining padding may influence RNN state use post and pad with values not in dataset (don't use 0 if in ds)\n",
    "X_pad = pad_sequences(X, dtype='float32', padding='post', value=-1000)\n",
    "# Use Masking to tell RNN to discard these values on foward/backward pass\n",
    "from tensorflow.keras.layers import Masking\n",
    "model.add(layers.Masking(mask_value=-1000, input_shape=(4,3))) # Put this after sequence in pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Example code\n",
    "\n",
    "\n",
    "# Time Serie split if df is one ts\n",
    "len_ = int(0.8*df.shape[0])\n",
    "df_train = df[:len_]\n",
    "df_test = df[len_:]\n",
    "\n",
    "# Scale with MinMaxScale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "columns = df.columns\n",
    "scaler = MinMaxScaler()\n",
    "df_train = pd.DataFrame(scaler.fit_transform(df_train), columns=columns)\n",
    "df_test = pd.DataFrame(scaler.transform(df_test), columns=columns)\n",
    "df_train.head()\n",
    "\n",
    "# Functions for analysis\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def subsample_sequence(df, length):\n",
    "    \"\"\"\n",
    "    Given the initial dataframe `df`, return a shorter dataframe sequence of length `length`.\n",
    "    This shorter sequence should be selected at random.\n",
    "    \"\"\"\n",
    "    \n",
    "    last_possible = df.shape[0] - length\n",
    "    \n",
    "    random_start = np.random.randint(0, last_possible)\n",
    "    df_sample = df[random_start: random_start+length]\n",
    "    \n",
    "    return df_sample\n",
    "\n",
    "def compute_means(X, df_mean):\n",
    "    '''utils'''\n",
    "    # Compute means of X\n",
    "    means = X.mean()\n",
    "    \n",
    "    # Case if ALL values of at least one feature of X are NaN, then reaplace with the whole df_mean\n",
    "    if means.isna().sum() != 0:\n",
    "        means.fillna(df_mean, inplace=True)\n",
    "        \n",
    "    return means\n",
    "\n",
    "def split_subsample_sequence(df, length, df_mean=None):\n",
    "    \"\"\"Return one single sample (Xi, yi) containing one sequence each of length `length`\"\"\"\n",
    "    features_names = ['TEMP', 'DEWP', 'PRES', 'Ir', 'Is', 'Iws']\n",
    "    \n",
    "    # Trick to save time during the recursive calls\n",
    "    if df_mean is None:\n",
    "        df_mean = df[features_names].mean()\n",
    "        \n",
    "    df_subsample = subsample_sequence(df, length).copy()\n",
    "    \n",
    "    # Let's drop any row without a target! We need targets to fit our model\n",
    "    df_subsample.dropna(how='any', subset=['pm2.5'], inplace=True)\n",
    "    \n",
    "    # Create y_sample\n",
    "    if df_subsample.shape[0] == 0: # Case if there is no targets at all remaining\n",
    "        return split_subsample_sequence(df, length, df_mean) # Redraw by recursive call until it's not the case anymore\n",
    "    y_sample = df_subsample[['pm2.5']]\n",
    "    \n",
    "    # Create X_sample\n",
    "    X_sample = df_subsample[features_names]\n",
    "    if X_sample.isna().sum().sum() !=0:  # Case X_sample has some NaNs\n",
    "        X_sample = X_sample.fillna(compute_means(X_sample, df_mean))\n",
    "        \n",
    "    return np.array(X_sample), np.array(y_sample)\n",
    "\n",
    "def get_X_y(df, sequence_lengths):\n",
    "    '''Return a dataset (X, y)'''\n",
    "    X, y = [], []\n",
    "\n",
    "    for length in sequence_lengths:\n",
    "        xi, yi = split_subsample_sequence(df, length)\n",
    "        X.append(xi)\n",
    "        y.append(yi)\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "# Here we define the parameter to generate our train/test sets\n",
    "train_size = 1000\n",
    "test_size = round(0.2 * train_size)\n",
    "min_seq_len = 15\n",
    "max_seq_len = 30\n",
    "\n",
    "# Let's generate some train/test sets\n",
    "sequence_lengths_train = np.random.randint(low=min_seq_len, high=max_seq_len, size=train_size)\n",
    "X_train, y_train = get_X_y(df_train, sequence_lengths_train)\n",
    "sequence_lengths_test = np.random.randint(low=min_seq_len, high=max_seq_len, size=test_size)\n",
    "X_test, y_test = get_X_y(df_test, sequence_lengths_test)\n",
    "\n",
    "# Test if there are no NaN in you dataset\n",
    "assert np.sum([np.isnan(x[0]).sum() for x in X_train]) == 0\n",
    "assert np.sum([np.isnan(x[0]).sum() for x in X_test]) == 0\n",
    "assert np.sum([np.isnan(y[0]).sum() for y in y_train]) == 0\n",
    "assert np.sum([np.isnan(y[0]).sum() for y in y_test]) == 0\n",
    "\n",
    "# Check your shapes\n",
    "print(\"X_train type\", type(X_train))\n",
    "print(\"n_sequence\", len(X_train))\n",
    "print(\"shape sequence 0\", X_train[0].shape)\n",
    "print(\"shape sequence 1\", X_train[1].shape)\n",
    "print(\"...\")\n",
    "print(\"\\n\")\n",
    "print(\"y_train type\", type(y_train))\n",
    "print(\"n_sequence\", len(X_train))\n",
    "print(\"shape sequence 0\", X_train[0].shape)\n",
    "print(\"shape sequence 1\", X_train[1].shape)\n",
    "print(\"...\")\n",
    "\n",
    "# Padding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_train_pad = pad_sequences(X_train, value=-1000., dtype=float, padding='post', maxlen=30)\n",
    "y_train_pad = pad_sequences(y_train, value=-1000., dtype=float, padding='post', maxlen=30)\n",
    "X_test_pad = pad_sequences(X_test, value=-1000., dtype=float, padding='post', maxlen=30)\n",
    "y_test_pad = pad_sequences(y_test, value=-1000., dtype=float, padding='post', maxlen=30)\n",
    "\n",
    "# If needing to add another dimension\n",
    "#X_pad = np.expand_dims(X_pad, 2)\n",
    "\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "#y_cat = to_categorical(y)\n",
    "#y_cat.shape\n",
    "\n",
    "# We here decide to predict the mean of the train set y_train\n",
    "y_pred = np.mean([_ for elt in y_train for _ in elt])\n",
    "bench_res = np.mean(np.abs([_-y_pred for elt in y_test for _ in elt]))\n",
    "print(f'Benchmark MAE on the test set : {bench_res:.4f}')\n",
    "\n",
    "# Generate model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "def init_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Masking(mask_value=-1000., input_shape=(30,6)))\n",
    "    model.add(layers.LSTM(20, return_sequences=True, activation='tanh'))\n",
    "    model.add(layers.LSTM(10, return_sequences=True, activation='tanh'))\n",
    "    model.add(layers.Dense(5, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', \n",
    "                  optimizer='rmsprop', \n",
    "                  metrics=['mae'])\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=RMSprop(learning_rate=0.0003), \n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "init_model().summary()\n",
    "\n",
    "# Fit model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = init_model()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_pad, y_train_pad,\n",
    "            validation_split=0.2,\n",
    "            epochs=1000, \n",
    "            batch_size=64,\n",
    "            callbacks=[es], verbose=0)\n",
    "\n",
    "# Evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "res = model.evaluate(X_test_pad, y_test_pad, verbose=0)\n",
    "print(f'MAE on the test set : {res[1]:.4f}, or {res[1]/bench_res:.2f} times the benchmark')\n",
    "\n",
    "# Plot many true sequences, as well as their predictions, to visualize your model performance\n",
    "plt.figure(figsize=(17,3))\n",
    "for id_plot, id_seq in enumerate(np.random.randint(low=0,high=len(X_test),size=5)):\n",
    "    plt.subplot(1, 5, id_plot+1)\n",
    "    plt.plot(y_test[id_seq], label='truth')\n",
    "    plt.plot(model.predict(X_test_pad)[id_seq], label='pred')\n",
    "    plt.ylim(ymin=0)\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1afbb3",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f09dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To do sentiment analysis make X the text and y the sentiment. Use this to train a model then give it\n",
    "text to predict the sentiment on. To predict next character give it X and make y the next character.'''\n",
    "\n",
    "# Do preprocessing steps as before\n",
    "\n",
    "# 1. Tokenize the vocabulary \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X)\n",
    "vocab_size = len(tk.word_index)\n",
    "X_token = tk.texts_to_sequences(X)\n",
    "#print(f'There are {vocab_size} different words in your corpus')\n",
    "\n",
    "# 2. Check sentence lengths (for padding)\n",
    "plt.hist([len(x) for x in X_tokens])\n",
    "plt.show()\n",
    "\n",
    "# 3. Pad the inputs (set maxlen smaller to make runtime quicker)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_pad = pad_sequences(X_token, dtype='float32', padding='post', maxlen=50) # max sentence len\n",
    "#X_train_pad = pad_sequences(X_train, dtype='float32', padding='post')\n",
    "#X_test_pad = pad_sequences(X_test, dtype='float32', padding='post')\n",
    "\n",
    "# 4. Check the length of X_pad against vocab_size\n",
    "print(len(np.unique(X_pad)))\n",
    "print(vocab_size)\n",
    "\n",
    "# 5. Baseline MAE score, when always predicting the mean price\n",
    "y_mean = y.mean()\n",
    "np.mean(np.abs(y - y_mean))\n",
    "\n",
    "# 6. Embedding and model building with RNN or CNN- For Word2Vec see below\n",
    "embedding_size = 20 # dimensions that the tokenized words will be vectorised on\n",
    "\n",
    "from tensorflow.keras import layers, models, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model_rnn():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        input_length=maxlen, # max sentence len\n",
    "        output_dim=embedding_size,\n",
    "        mask_zero=True\n",
    "    ))\n",
    "    model.add(layers.LSTM(10, return_sequences=True))\n",
    "    model.add(layers.LSTM(10))\n",
    "    model.add(layers.Dense(30, activation='relu'))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.0005), metrics=['mae'])\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Replace LSTM with these to use CNN\n",
    "#model.add(layers.Conv1D(10, kernel_size=15, padding='same', activation=\"relu\"))\n",
    "#model.add(layers.Conv1D(10, kernel_size=10, padding='same', activation=\"relu\"))\n",
    "#model.add(layers.Flatten())\n",
    "\n",
    "model_rnn = build_model_rnn()\n",
    "model_rnn.summary()\n",
    "\n",
    "# 7. Run the model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train, \n",
    "          batch_size = 32,\n",
    "          epochs=100,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n",
    "\n",
    "# 8. Evaluate the model\n",
    "res = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')\n",
    "\n",
    "\n",
    "# Training Word2Vec on data\n",
    "from gensim.models import Word2Vec\n",
    "# vector_size corresponds to embedding space, min_count= min times to be included, window= numbe of surrounding words taken into account\n",
    "word2vec = Word2Vec(sentences=X_train, vector_size=50, min_count=40, window=10)\n",
    "wv = word2vec.wv\n",
    "\n",
    "# Show the vector co-ordinates of dog\n",
    "wv['dog']\n",
    "# Show the embedding space\n",
    "len(wv['cat'])\n",
    "# Show the word to token values\n",
    "wv.key_to_index\n",
    "# Show most similar words\n",
    "wv.most_similar(\"car\") # Show by word directly\n",
    "wv.similar_by_vector(wv['car']) # Show by vector directly\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])   \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=200)\n",
    "\n",
    "\n",
    "\n",
    "# Transfer learning with Word2Vec\n",
    "\n",
    "import gensim.downloader as api\n",
    "# This will show you the available models\n",
    "print(list(api.info()['models'].keys()))\n",
    "# Load the model you want\n",
    "word2vec_transfer = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_2 = embedding(word2vec_transfer, X_test)\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=200)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=200)\n",
    "\n",
    "# Then fit and evaluate as above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6d7cc",
   "metadata": {},
   "source": [
    "Predictive text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ace93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function that will take in text and return X (beginning of string) and y (following character)\n",
    "import numpy as np\n",
    "\n",
    "def get_X_y(string, length=300):\n",
    "    if len(string) <= length:\n",
    "        return None\n",
    "    first_letter_idx = np.random.randint(0, len(string)-length)\n",
    "    X_letters = string[first_letter_idx:first_letter_idx+length]\n",
    "    y_letter = string[first_letter_idx+length]\n",
    "    return X_letters, y_letter\n",
    "\n",
    "# Function that will return X and y when given a list of sentences\n",
    "def create_dataset(sentences):\n",
    "    X, y = [], []\n",
    "    number_of_samples = 20000\n",
    "    indices = np.random.randint(0, len(sentences), size=number_of_samples)\n",
    "    for idx in indices:\n",
    "        ret = get_X_y(sentences[idx])\n",
    "        if ret is None:\n",
    "            continue\n",
    "        xi, yi = ret\n",
    "        X.append(xi)\n",
    "        y.append(yi)\n",
    "    return X, y\n",
    "\n",
    "X, y = create_dataset(X)\n",
    "\n",
    "# Train test split\n",
    "len_ = int(0.7*len(X))\n",
    "string_train = X[:len_]\n",
    "string_test = X[len_:]\n",
    "y_train = y[:len_]\n",
    "y_test = y[len_:]\n",
    "\n",
    "# Create a dictionary which stores a unique token for each letter (key= letter, value = corresponding token)\n",
    "letter_to_id = {}\n",
    "letter_to_id['UNKNOWN'] = 0\n",
    "\n",
    "iter_ = 1\n",
    "for string in string_train:\n",
    "    for letter in string:\n",
    "        if letter in letter_to_id:\n",
    "            continue\n",
    "        letter_to_id[letter] = iter_\n",
    "        iter_ += 1\n",
    "        \n",
    "for string in y_train:\n",
    "    for letter in string:\n",
    "        if letter in letter_to_id:\n",
    "            continue\n",
    "        letter_to_id[letter] = iter_\n",
    "        iter_ += 1\n",
    "\n",
    "# Tokenize the str\n",
    "X_train = [[letter_to_id[_] for _ in x] for x in string_train]\n",
    "X_test = [[letter_to_id[_] if _ in letter_to_id else letter_to_id['UNKNOWN'] for _ in x ] for x in string_test]\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Tokenize the letters\n",
    "y_train_token = [letter_to_id[x] for x in y_train]\n",
    "y_test_token = [letter_to_id[x] if x in letter_to_id else letter_to_id['UNKNOWN'] for x in y_test]\n",
    "\n",
    "# Use ohe\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train_cat = to_categorical(y_train_token, num_classes=len(letter_to_id))\n",
    "y_test_cat = to_categorical(y_test_token, num_classes=len(letter_to_id))\n",
    "\n",
    "# Print baseline model\n",
    "from sklearn.metrics import accuracy_score\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "counts = dict(zip(unique, counts))\n",
    "print('Number of labels in train set', counts)\n",
    "w = -1\n",
    "y_pred = ''\n",
    "for k, v in counts.items():\n",
    "    if v > w:\n",
    "        y_pred = k\n",
    "        w = v\n",
    "\n",
    "print('Baseline accuracy: ', accuracy_score(y_test, [y_pred]*len(y_test)))\n",
    "\n",
    "# Generate model\n",
    "from tensorflow.keras import Sequential, layers\n",
    "def init_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=30))\n",
    "    model.add(layers.GRU(30, activation='tanh'))\n",
    "    model.add(layers.Dense(30, activation='relu'))\n",
    "    model.add(layers.Dense(vocab_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = init_model(len(letter_to_id))\n",
    "model.summary()\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, monitor='val_loss')\n",
    "\n",
    "# Fit model\n",
    "model = init_model(len(letter_to_id))\n",
    "model.fit(X_train, y_train_cat,\n",
    "          epochs=3, \n",
    "          batch_size=50,\n",
    "          callbacks=[es],\n",
    "          validation_split=0.3)\n",
    "\n",
    "# Evaluate model\n",
    "model.evaluate(X_test, y_test_cat)\n",
    "\n",
    "# Predict the next letter\n",
    "id_to_letter = {v: k for k, v in letter_to_id.items()}\n",
    "\n",
    "def get_predicted_letter(string):\n",
    "    string_convert = [letter_to_id[_] for _ in string]\n",
    "    pred = model.predict([string_convert])\n",
    "    pred_class = np.argmax(pred[0])\n",
    "    pred_letter = id_to_letter[pred_class]\n",
    "    return pred_letter\n",
    "\n",
    "string = 'this is a good'\n",
    "get_predicted_letter(string)\n",
    "\n",
    "# Predict several characters by appending the pred from above and then predicting again\n",
    "def repeat_prediction(string, repetition):\n",
    "    string_tmp = string\n",
    "    for i in range(repetition):\n",
    "        predicted_letter = get_predicted_letter(string_tmp)\n",
    "        string_tmp = string_tmp + predicted_letter\n",
    "    return string_tmp\n",
    "\n",
    "strings = ['what i like is ',\n",
    "          ]\n",
    "[repeat_prediction(string, 20) for string in strings]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
